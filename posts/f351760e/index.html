<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover"><title>kafka基础知识 | 小五的个人杂货铺</title><meta name="author" content="小五"><meta name="copyright" content="小五"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="Kafka概述定义 Kafka传统定义：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（MessageQueue），主要应用于大数据实时处理领域。  发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息。 Kafka最新定义：Kafka是一个开源的分布式事件流平台（Event StreamingPlatform"><meta property="og:type" content="article"><meta property="og:title" content="kafka基础知识"><meta property="og:url" content="https://xiaowu95.wang/posts/f351760e/"><meta property="og:site_name" content="小五的个人杂货铺"><meta property="og:description" content="Kafka概述定义 Kafka传统定义：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（MessageQueue），主要应用于大数据实时处理领域。  发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息。 Kafka最新定义：Kafka是一个开源的分布式事件流平台（Event StreamingPlatform"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://xiaowu95.wang/img/nba-logo19.jpg"><meta property="article:published_time" content="2023-03-23T16:00:00.000Z"><meta property="article:modified_time" content="2024-02-28T08:37:47.000Z"><meta property="article:author" content="小五"><meta property="article:tag" content="kafka"><meta property="article:tag" content="mq"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://xiaowu95.wang/img/nba-logo19.jpg"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="canonical" href="https://xiaowu95.wang/posts/f351760e/"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""><link rel="preconnect" href="//hm.baidu.com"><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="google-site-verification" content="kHfLZxYQ7y3s3AR7gyJDdJkQvGNjQsvopp6N3gEEx0s"><meta name="baidu-site-verification" content="codeva-aVS4F6wAjS"><link rel="manifest" href="/wang-xiaowu/pwa/site.webmanifest"><meta name="msapplication-TileColor" content="#fff"><link rel="apple-touch-icon" sizes="180x180" href="/wang-xiaowu/pwa/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/wang-xiaowu/pwa/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/wang-xiaowu/pwa/favicon-16x16.png"><link rel="mask-icon" href="/wang-xiaowu/pwa/safari-pinned-tab.svg" color="#5bbad5"><link rel="stylesheet" href="/css/index.css?v=5.2.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.6.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.36/dist/fancybox/fancybox.min.css" media="print" onload='this.media="all"'><script>(()=>{const t={set:(e,t,o)=>{if(!o)return;const a=Date.now()+864e5*o;localStorage.setItem(e,JSON.stringify({value:t,expiry:a}))},get:e=>{const t=localStorage.getItem(e);if(!t)return;const{value:o,expiry:a}=JSON.parse(t);if(!(Date.now()>a))return o;localStorage.removeItem(e)}};window.btf={saveToLocal:t,getScript:(e,t={})=>new Promise(((o,a)=>{const n=document.createElement("script");n.src=e,n.async=!0,Object.entries(t).forEach((([e,t])=>n.setAttribute(e,t))),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)})),getCSS:(e,t)=>new Promise(((o,a)=>{const n=document.createElement("link");n.rel="stylesheet",n.href=e,t&&(n.id=t),n.onload=n.onreadystatechange=()=>{n.readyState&&!/loaded|complete/.test(n.readyState)||o()},n.onerror=a,document.head.appendChild(n)})),addGlobalFn:(e,t,o=!1,a=window)=>{const n=a.globalFn||{};n[e]=n[e]||{},n[e][o||Object.keys(n[e]).length]=t,a.globalFn=n}};const o=()=>{document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},a=()=>{document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};btf.activateDarkMode=o,btf.activateLightMode=a;const n=t.get("theme"),c=window.matchMedia("(prefers-color-scheme: dark)"),r=window.matchMedia("(prefers-color-scheme: light)");if(void 0===n){if(r.matches)a();else if(c.matches)o();else{const e=(new Date).getHours();e<=6||e>=18?o():a()}c.addEventListener("change",(()=>{void 0===t.get("theme")&&(e.matches?o():a())}))}else"light"===n?a():o();const d=t.get("aside-status");void 0!==d&&document.documentElement.classList.toggle("hide-aside","hide"===d);/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)&&document.documentElement.classList.add("apple")})()</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?86cb34987e15c900c39e12ebdf08c50d";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}(),btf.addGlobalFn("pjaxComplete",(()=>{_hmt.push(["_trackPageview",window.location.pathname])}),"baidu_analytics")</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MFQ47191J"></script><script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-1MFQ47191J"),btf.addGlobalFn("pjaxComplete",(()=>{gtag("config","G-1MFQ47191J",{page_path:window.location.pathname})}),"google_analytics")</script><script>const GLOBAL_CONFIG={root:"/",algolia:{appId:"M91QAWRVY4",apiKey:"ecc30a5e06136e1cc491685e39801190",indexName:"xiaowu-blog",hitsPerPage:6,languages:{input_placeholder:"搜索文章",hits_empty:"未找到符合您查询的内容：${query}",hits_stats:"找到 ${hits} 条结果，耗时 ${time} 毫秒"}},localSearch:void 0,translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},highlight:{plugin:"highlight.js",highlightCopy:!0,highlightLang:!0,highlightHeightLimit:400,highlightFullpage:!0,highlightMacStyle:!0},copy:{success:"复制成功",error:"复制失败",noSupport:"浏览器不支持"},relativeDate:{homepage:!0,post:!0},runtime:"天",dateSuffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:{limitCount:150,languages:{author:"作者: 小五",link:"链接: ",source:"来源: 小五的个人杂货铺",info:"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},lightbox:"fancybox",Snackbar:{chs_to_cht:"已切换为繁体中文",cht_to_chs:"已切换为简体中文",day_to_night:"已切换为深色模式",night_to_day:"已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#1f1f1f",position:"bottom-left"},infinitegrid:{js:"https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.12.0/dist/infinitegrid.min.js",buttonText:"加载更多"},isPhotoFigcaption:!1,islazyload:!0,isAnchor:!0,percent:{toc:!0,rightside:!0},autoDarkmode:!0}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={title:"kafka基础知识",isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,isShuoshuo:!1}</script><script src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><link rel="stylesheet" href="/css/font.css"><style>.app-refresh{position:fixed;top:-2.2rem;left:0;right:0;z-index:99999;padding:0 1rem;font-size:15px;height:2.2rem;transition:all .3s ease}.app-refresh-wrap{display:flex;color:#fff;height:100%;align-items:center;justify-content:center}.app-refresh-wrap a{color:#fff;text-decoration:underline;cursor:pointer}</style><link rel="stylesheet" href="/css/titleStyle.css"><link rel="stylesheet" href="/css/mouse.css"><link rel="stylesheet" href="/css/custom.css"><style>.card-announcement .social-button{margin:.6rem 0 0 0;text-align:center}.card-announcement .social-button a{display:block;background-color:var(--btn-bg);color:var(--btn-color);text-align:center;line-height:2.4;margin:4px 0}.card-announcement .social-button a:hover{background-color:var(--btn-hover-color)}</style><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper/swiper/swiper.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper/swiper/swiperstyle.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/css/main.css"><link rel="alternate" href="/atom.xml" title="小五的个人杂货铺" type="application/atom+xml"></head><body><script>window.paceOptions={restartOnPushState:!1},btf.addGlobalFn("pjaxSend",(()=>{Pace.restart()}),"pace_restart")</script><link rel="stylesheet" href="/css/pace.css"><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img text-center"><img src="/img/loading.gif" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data text-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">550</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">163</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">58</div></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/github/"><i class="fa-fw fa-solid fa-address-card"></i> <span>关于我</span></a></div><div class="menus_item"><a class="site-page" href="/ac/"><i class="fa-fw fa-solid fa-wind"></i> <span>便携小空调</span></a></div><div class="menus_item"><a class="site-page" href="/talking/"><i class="fa-fw fa-solid fa-walkie-talkie"></i> <span>博客短记</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-compass"></i> <span>目录</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-otter"></i> <span>杂记</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/songs/"><i class="fa-fw fas fa-music"></i> <span>音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i> <span>图库</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fa-solid fa-film"></i> <span>影视|番剧</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book faa-fload"></i> <span>书单</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad faa-fload"></i> <span>游戏</span></a></li><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fa-brands fa-bilibili"></i> <span>BiliBili追番</span></a></li><li><a class="site-page child" href="/cinemas/"><i class="fa-fw fa-solid fa-video"></i> <span>BiliBili追剧</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comment-dots"></i> <span>留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa-solid fa-user-group"></i> <span>友链</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-mug-saucer"></i> <span>Website Memo</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://console.leancloud.app/apps"><span>🚀 LeanCloud</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://vercel.com/dashboard"><span>🚀 Vercel</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://tongji.baidu.com/main/overview/10000432799/overview/index"><span>🚀 百度统计</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://ziyuan.baidu.com/dashboard/index"><span>🚀 百度站点管理</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://analytics.google.com/analytics/web/"><span>🚀 谷歌分析</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://search.google.com/search-console?hl=zh-cn"><span>🚀 谷歌站点管理</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.google.com/adsense/new/u/0/pub-1375421173355613/home"><span>🚀 谷歌广告联盟</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.algolia.com/"><span>🚀 Algolia</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://sms-activate.org/cn/"><span>🚀 Sms-activate</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://chatgpt.com/"><span>🚀 ChatGPT</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://poe.com/"><span>🚀 Poe聚合</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="http://www.idc.net/clientarea"><span>🚀 后浪云</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.west.cn/"><span>🚀 西部数据</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://shandianpro.com/#/dashboard"><span>🚀 闪电</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.huojian.homes/"><span>🚀 小火箭</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/winston779/gougou"><span>🚀 狗狗加速</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(/img/nba-logo19.jpg)"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">小五的个人杂货铺</span></a><a class="nav-page-title" href="/"><span class="site-name">kafka基础知识</span></a></span><div id="menus"><div id="search-button"><span class="site-page social-icon search"><i class="fas fa-search fa-fw"></i> <span>搜索</span></span></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i> <span>首页</span></a></div><div class="menus_item"><a class="site-page" href="/github/"><i class="fa-fw fa-solid fa-address-card"></i> <span>关于我</span></a></div><div class="menus_item"><a class="site-page" href="/ac/"><i class="fa-fw fa-solid fa-wind"></i> <span>便携小空调</span></a></div><div class="menus_item"><a class="site-page" href="/talking/"><i class="fa-fw fa-solid fa-walkie-talkie"></i> <span>博客短记</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fas fa-compass"></i> <span>目录</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/archives/"><i class="fa-fw fas fa-archive"></i> <span>归档</span></a></li><li><a class="site-page child" href="/tags/"><i class="fa-fw fas fa-tags"></i> <span>标签</span></a></li><li><a class="site-page child" href="/categories/"><i class="fa-fw fas fa-folder-open"></i> <span>分类</span></a></li></ul></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-otter"></i> <span>杂记</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" href="/songs/"><i class="fa-fw fas fa-music"></i> <span>音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i> <span>图库</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fa-solid fa-film"></i> <span>影视|番剧</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book faa-fload"></i> <span>书单</span></a></li><li><a class="site-page child" href="/games/"><i class="fa-fw fas fa-gamepad faa-fload"></i> <span>游戏</span></a></li><li><a class="site-page child" href="/bangumis/"><i class="fa-fw fa-brands fa-bilibili"></i> <span>BiliBili追番</span></a></li><li><a class="site-page child" href="/cinemas/"><i class="fa-fw fa-solid fa-video"></i> <span>BiliBili追剧</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/comments/"><i class="fa-fw fas fa-comment-dots"></i> <span>留言板</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa-solid fa-user-group"></i> <span>友链</span></a></div><div class="menus_item"><span class="site-page group"><i class="fa-fw fa-solid fa-mug-saucer"></i> <span>Website Memo</span><i class="fas fa-chevron-down"></i></span><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://console.leancloud.app/apps"><span>🚀 LeanCloud</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://vercel.com/dashboard"><span>🚀 Vercel</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://tongji.baidu.com/main/overview/10000432799/overview/index"><span>🚀 百度统计</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://ziyuan.baidu.com/dashboard/index"><span>🚀 百度站点管理</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://analytics.google.com/analytics/web/"><span>🚀 谷歌分析</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://search.google.com/search-console?hl=zh-cn"><span>🚀 谷歌站点管理</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.google.com/adsense/new/u/0/pub-1375421173355613/home"><span>🚀 谷歌广告联盟</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.algolia.com/"><span>🚀 Algolia</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://sms-activate.org/cn/"><span>🚀 Sms-activate</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://chatgpt.com/"><span>🚀 ChatGPT</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://poe.com/"><span>🚀 Poe聚合</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="http://www.idc.net/clientarea"><span>🚀 后浪云</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.west.cn/"><span>🚀 西部数据</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://shandianpro.com/#/dashboard"><span>🚀 闪电</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://www.huojian.homes/"><span>🚀 小火箭</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/winston779/gougou"><span>🚀 狗狗加速</span></a></li></ul></div></div><div id="toggle-menu"><span class="site-page"><i class="fas fa-bars fa-fw"></i></span></div></div></nav><div id="post-info"><h1 class="post-title">kafka基础知识<a class="post-edit-link" href="https://github.com/wang-xiaowu/wang-xiaowu.github.io/issues/new?_posts/kafka/kafka基础知识.md" rel="external nofollow noreferrer" title="编辑" target="_blank"><i class="fas fa-pencil-alt"></i></a></h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-23T16:00:00.000Z" title="发表于 2023-03-24 00:00:00">2023-03-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-02-28T08:37:47.000Z" title="更新于 2024-02-28 16:37:47">2024-02-28</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/kafka/">kafka</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">总字数:</span><span class="word-count">24.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>92分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><div id="post-outdate-notice" data="{&quot;limitDay&quot;:365,&quot;messagePrev&quot;:&quot;距离上一次更新该文章已经过了&quot;,&quot;messageNext&quot;:&quot;天，文章所描述的內容可能已经发生变化，请留意。&quot;,&quot;postUpdate&quot;:&quot;2024-02-28 16:37:47&quot;}" hidden></div><h1 id="Kafka概述"><a href="#Kafka概述" class="headerlink" title="Kafka概述"></a>Kafka概述</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>Kafka传统定义：Kafka是一个分布式的基于发布&#x2F;订阅模式的消息队列（MessageQueue），主要应用于大数据实时处理领域。</p><p>发布&#x2F;订阅：消息的发布者不会将消息直接发送给特定的订阅者，而是将发布的消息分为不同的类别，订阅者只接收感兴趣的消息。</p><p>Kafka最新定义：Kafka是一个开源的分布式事件流平台（Event StreamingPlatform），被数千家公司用于高性能数据管道、流分析、数据集成和关键任务应用。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/dfb2e94613dfe8ca28b1f786cdbd1cad.png" title="image-20220902124022501"><h2 id="消息队列"><a href="#消息队列" class="headerlink" title="消息队列"></a>消息队列</h2><p>目前企业中比较常见的消息队列产品主要有 Kafka、ActiveMQ 、RabbitMQ 、RocketMQ 等。</p><p>在大数据场景主要采用 Kafka 作为消息队列。在 JavaEE 开发中主要采用 ActiveMQ、RabbitMQ、RocketMQ。</p><h3 id="传统消息队列的应用场景"><a href="#传统消息队列的应用场景" class="headerlink" title="传统消息队列的应用场景"></a>传统消息队列的应用场景</h3><p>传统的消息队列的主要应用场景包括：<strong>缓存消峰</strong>、<strong>解耦</strong>和<strong>异步通信。</strong></p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/77a7834185b244829c98edb00813ac57.png" title="image-20220902124743197"> <img src="/img/loading.gif" data-lazy-src="/posts/f351760e/ef3b5d2ae3fecd1d46e41c08840024ae.png" title="image-20220902124806543"> <img src="/img/loading.gif" data-lazy-src="/posts/f351760e/56d9b13e9919a3b9d80b70f9ff98c39f.png" title="image-20220902124830120"><h3 id="消息队列的两种模式"><a href="#消息队列的两种模式" class="headerlink" title="消息队列的两种模式"></a>消息队列的两种模式</h3><p>消息队列的两种模式</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/87cbfaac48e317d6a7648b2e66f9941b.png" title="image-20220902124921601"><p>区别: 点对点消费 -&gt; 消息只能发布到一个主题， 消费完成就删除消息，且只有一个消费者</p><p>发布订阅模式 -&gt; 消息可以发布到多个主题， 消息一般保留七天，且有多个消费者</p><h2 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h2><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/708e86e70504f41234b05cb3cc30dea7.png" title="image-20220902125656203"><p>在Kafka2.8版本前，Zookeeper的Consumer文件中存放消息被消费的记录（offset）</p><p>在Kafka2.8版本走，消息被消费的记录（offset）存放在Kafka中。</p><p>（1）Producer：消息生产者，就是向 Kafka broker 发消息的客户端。</p><p>（2）Consumer：消息消费者，向 Kafka broker 取消息的客户端。</p><p>（3）Consumer Group（CG）：消费者组，由多个 consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</p><p>（4）Broker：一台 Kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个broker 可以容纳多个 topic。</p><p>（5）Topic：可以理解为一个队列，生产者和消费者面向的都是一个 topic。</p><p>（6）Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列。</p><p>（7）Replica：副本。一个 topic 的每个分区都有若干个副本，一个 Leader 和若干个Follower。</p><p>（8）Leader：每个分区多个副本的“主”，<strong>生产者发送数据的对象，以及消费者消费数据的对象都是 Leader。</strong></p><p>（9）Follower：每个分区多个副本中的“从”，实时从 Leader 中同步数据，保持和Leader 数据的同步。Leader 发生故障时，某个 Follower 会成为新的 Leader。</p><h1 id="Kafka-快速入门"><a href="#Kafka-快速入门" class="headerlink" title="Kafka 快速入门"></a>Kafka 快速入门</h1><h2 id="安装部署"><a href="#安装部署" class="headerlink" title="安装部署"></a>安装部署</h2><h3 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h3><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/7a6de8540a14d9de14d5d3349aa6cf37.png" title="image-20220902130143941"><p><strong>2.1.2</strong> <strong>集群部署</strong></p><p>0）官方下载地址：<a target="_blank" rel="noopener external nofollow noreferrer" href="http://kafka.apache.org/downloads.html">http://kafka.apache.org/downloads.html</a></p><p>1）解压安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 software]$ tar -zxvf kafka_2.12-3.0.0.tgz -C /opt/module/</span><br></pre></td></tr></table></figure><p>2)修改解压后的文件名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ mv kafka_2.12-3.0.0/ kafka</span><br></pre></td></tr></table></figure><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/1ff6b0c3ba569bdcdc10f5db72de9f25.png" title="image-20220902151810746"><p>3）进入到&#x2F;opt&#x2F;module&#x2F;kafka 目录，修改配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ cd config/</span><br><span class="line">[atguigu@hadoop102 config]$ vim server.properties</span><br></pre></td></tr></table></figure><p>输入以下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">broker 的全局唯一编号，不能重复，只能是数字。</span></span><br><span class="line">broker.id=0</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">处理网络请求的线程数量</span></span><br><span class="line">num.network.threads=3</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">用来处理磁盘 IO 的线程数量</span></span><br><span class="line">num.io.threads=8</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">发送套接字的缓冲区大小</span></span><br><span class="line">socket.send.buffer.bytes=102400</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">接收套接字的缓冲区大小</span></span><br><span class="line">socket.receive.buffer.bytes=102400</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">请求套接字的缓冲区大小</span></span><br><span class="line">socket.request.max.bytes=104857600</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kafka 运行日志(数据)存放的路径，路径不需要提前创建，kafka 自动帮你创建，可以配置多个磁盘路径，路径与路径之间可以用<span class="string">&quot;，&quot;</span>分隔</span></span><br><span class="line">log.dirs=/opt/module/kafka/datas</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">topic 在当前 broker 上的分区个数</span></span><br><span class="line">num.partitions=1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">用来恢复和清理 data 下数据的线程数量</span></span><br><span class="line">num.recovery.threads.per.data.dir=1</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">每个 topic 创建时的副本数，默认时 1 个副本</span></span><br><span class="line">offsets.topic.replication.factor=1</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">segment 文件保留的最长时间，超时将被删除</span></span><br><span class="line">log.retention.hours=168</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">每个 segment 文件的大小，默认最大 1G</span></span><br><span class="line">log.segment.bytes=1073741824</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">检查过期数据的时间，默认 5 分钟检查一次是否数据过期</span></span><br><span class="line">log.retention.check.interval.ms=300000</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">配置连接 Zookeeper 集群地址（在 zk 根目录下创建/kafka，方便管理）</span></span><br><span class="line">zookeeper.connect=hadoop102:2181,hadoop103:2181,hadoop104:2181/kafka</span><br></pre></td></tr></table></figure><p>4）分发安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 module]$ xsync kafka/</span><br></pre></td></tr></table></figure><p>5）分别在 hadoop103 和 hadoop104 上修改配置文件&#x2F;opt&#x2F;module&#x2F;kafka&#x2F;config&#x2F;server.properties中的 broker.id&#x3D;1、broker.id&#x3D;2<br>注：broker.id 不得重复，整个集群中唯一。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop103 module]$ vim kafka/config/server.properties</span><br><span class="line">修改：</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">The <span class="built_in">id</span> of the broker. This must be <span class="built_in">set</span> to a unique <span class="built_in">integer</span> <span class="keyword">for</span> each broker.</span></span><br><span class="line">broker.id=1</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop104 module]$ vim kafka/config/server.properties</span><br><span class="line">修改:</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">The <span class="built_in">id</span> of the broker. This must be <span class="built_in">set</span> to a unique <span class="built_in">integer</span> <span class="keyword">for</span> each broker.</span></span><br><span class="line">broker.id=2</span><br></pre></td></tr></table></figure><p><strong>6）配置环境变量</strong><br>（1）在&#x2F;etc&#x2F;profile.d&#x2F;my_env.sh 文件中增加 kafka 环境变量配置</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile.d/my_env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment">#KAFKA_HOME</span></span><br><span class="line"><span class="built_in">export</span> KAFKA_HOME=/opt/module/kafka</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KAFKA_HOME</span>/bin</span><br></pre></td></tr></table></figure><p>刷新环境变量</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>（1） Zookeeper启动 (默认守护进程)</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./zkServer.sh start</span><br></pre></td></tr></table></figure><p>Zookeeper状态</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./zkServer.sh status</span><br></pre></td></tr></table></figure><p>Zookeeper停止</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./zkServer.sh stop</span><br><span class="line">    </span><br><span class="line">Zookeeper客户端-常⽤命令    </span><br><span class="line">./zkCli.sh –server ip:port  <span class="comment">//连接ZooKeeper服务端  </span></span><br><span class="line">quit  			<span class="comment">//断开连接    </span></span><br></pre></td></tr></table></figure><p>(2) 启动Kafka</p><p>Kafka 守护方式 (环境变量配置前提下)</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-start.sh -daemon /home/environment/kafka/config/server.properties</span><br></pre></td></tr></table></figure><p>Kafka关闭</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-stop.sh </span><br></pre></td></tr></table></figure><p><strong>注意：</strong>停止 Kafka 集群时，一定要等 Kafka 所有节点进程全部停止后再停止 Zookeeper集群。因为 Zookeeper 集群当中记录着 Kafka 集群相关信息，Zookeeper 集群一旦先停止，Kafka 集群就没有办法再获取停止进程的信息，只能手动杀死 Kafka 进程了。</p><h2 id="Kafka命令行操作"><a href="#Kafka命令行操作" class="headerlink" title="Kafka命令行操作"></a>Kafka命令行操作</h2><p><strong>基础结构</strong></p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/debd1eebe9291fcd3faaf7f1e4d235a2.png" title="image-20220902153447545"><h3 id="主题命令行操作"><a href="#主题命令行操作" class="headerlink" title="主题命令行操作"></a><strong>主题命令行操作</strong></h3><p>1）查看操作主题命令参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh</span><br></pre></td></tr></table></figure><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/6dcbf2af45419a358733602426cbd840.png" title="image-20220902153652112"><p>2）查看当前服务器中的所有 topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --list</span><br></pre></td></tr></table></figure><p>3）创建 first topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --create --partitions 1 --replication-factor 3 --topic first</span><br></pre></td></tr></table></figure><p>选项说明：<br><code>--topic</code> 定义 topic 名<br><code>--replication-factor</code> 定义副本数<br><code>--partitions</code> 定义分区数</p><p><strong>4）查看 first 主题的详情</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --alter --topic first --partitions 3</span><br></pre></td></tr></table></figure><p>5）修改分区数（注意：分区数只能增加，不能减少）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --alter --topic first --partitions 3</span><br></pre></td></tr></table></figure><p>6）再次查看 first 主题的详情</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --describe --topic first</span><br></pre></td></tr></table></figure><p>7）删除 topic(需要配置信息)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --delete --topic first</span><br></pre></td></tr></table></figure><h3 id="生产者命令行操作"><a href="#生产者命令行操作" class="headerlink" title="生产者命令行操作"></a>生产者命令行操作</h3><p>1）查看操作生产者命令参数</p><p>连接kafka生产者</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-producer.sh --bootstrap-server 47.106.86.64:9092 --topic first</span><br></pre></td></tr></table></figure><p>参数 描述<br><code>--bootstrap-server &lt;String: server toconnect to&gt;</code> 连接的 Kafka Broker 主机名称和端口号。<br><code>--topic &lt;String: topic&gt;</code> 操作的 topic 名称。</p><p>2）发送消息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh -- bootstrap-server hadoop102:9092 --topic first</span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">hello world</span></span><br><span class="line"><span class="meta prompt_">&gt;</span><span class="language-bash">Hi HI</span></span><br></pre></td></tr></table></figure><h3 id="消费者命令行操作"><a href="#消费者命令行操作" class="headerlink" title="消费者命令行操作"></a>消费者命令行操作</h3><p>1）查看操作消费者命令参数</p><p>连接kafka消费者</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh</span><br></pre></td></tr></table></figure><p>参数 描述<br>–bootstrap-server &lt;String: server toconnect to&gt; 连接的 Kafka Broker 主机名称和端口号。<br>–topic &lt;String: topic&gt; 操作的 topic 名称。<br>–from-beginning 从头开始消费。<br>–group &lt;String: consumer group id&gt; 指定消费者组名称。</p><p><strong>2）消费消息</strong><br>（1）消费 first 主题中的数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh --bootstrap-server 47.106.86.64:9092 --topic first</span><br></pre></td></tr></table></figure><p>（2）把主题中所有的数据都读取出来（包括历史数据）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh --bootstrap-server 47.106.86.64:9092 --from-beginning --topic first</span><br></pre></td></tr></table></figure><h1 id="Kafka-生产者"><a href="#Kafka-生产者" class="headerlink" title="Kafka 生产者"></a>Kafka 生产者</h1><h2 id="生产者消息发送流程"><a href="#生产者消息发送流程" class="headerlink" title="生产者消息发送流程"></a>生产者消息发送流程</h2><h3 id="发送原理"><a href="#发送原理" class="headerlink" title="发送原理"></a>发送原理</h3><p>在消息发送的过程中，涉及到两个线程，<strong>main线程</strong>和<strong>sender线程</strong>，其中main线程是消息的生产线程，而sender线程是jvm单例的线程，专门用于消息的发送。</p><p>在jvm的内存中开辟了一块缓存空间叫<strong>RecordAccumulator（消息累加器）</strong>，用于将多条消息合并成一个批次，然后由sender线程发送给kafka集群。</p><p>我们的一条消息在生产过程会调用<strong>send方法</strong>然后经过<strong>拦截器</strong>经过<strong>序列化器</strong>，再经过<strong>分区器</strong>确定消息发送在具体topic下的哪个分区，然后发送到对应的<strong>消息累加器</strong>中，消息累加器是多个双端队列。并且每个队列和主题分区都具有一一映射关系。消息在累加器中，进行合并，达到了对应的size（batch.size）或者等待超过对应的等待时间(linger.ms)，都会触发<strong>sender线程</strong>的发送。sender线程有一个请求池，默认缓存五个请求（ max.in.flight.requests.per.connection ），发送消息后，会等待服务端的ack，如果没收到ack就会重试默认重试int最大值（ retries ）。如果ack成功就会删除累加器中的消息批次，并相应到生产端。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/cd41370a872e70b75435f35692925370.png" title="image-20220902155220662"><p>当双端队列中的DQueue满足 batch.size 或者 linger.ms 条件时触发sender线程。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/8bccb405378132fdc00d8888fb3a29ea.png" title="img"><h3 id="生产者重要参数列表"><a href="#生产者重要参数列表" class="headerlink" title="生产者重要参数列表"></a><strong>生产者重要参数列表</strong></h3><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/650ad0e5593b68a62f099a9108b041c7.png" title="image-20220902160217190"><h2 id="发送"><a href="#发送" class="headerlink" title="发送"></a>发送</h2><h3 id="普通异步发送"><a href="#普通异步发送" class="headerlink" title="普通异步发送"></a>普通异步发送</h3><p>1）需求：创建 Kafka 生产者，采用异步的方式发送到 Kafka Broker</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/d69edce620624e1bfc5b64b340933144.png" title="img"> <img src="/img/loading.gif" data-lazy-src="/posts/f351760e/834a7285a77d8c9918af763e0ceeb475.png" title="在这里插入图片描述"><p>2）代码编写<br>（1）创建工程 kafka<br>（2）导入依赖</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>（4）编写不带回调函数的 API 代码</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducer</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        <span class="comment">//服务信息</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;47.106.86.64:9092&quot;</span>);</span><br><span class="line">        <span class="comment">//配置序列化</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br><span class="line">        <span class="comment">// 2. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String,String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;one&quot;</span> + i));</span><br><span class="line">        &#125;</span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试：在Linux上开启Kafka验证</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh --bootstrap-server 47.106.86.64:9092 --topic first</span><br></pre></td></tr></table></figure><h3 id="带回调函数的异步发送"><a href="#带回调函数的异步发送" class="headerlink" title="带回调函数的异步发送"></a>带回调函数的异步发送</h3><p>回调函数会在 producer 收到 ack 时调用，为异步调用，该方法有两个参数，分别是元数据信息（Record Metadata）和异常信息（Exception），如果 Exception 为 null，说明消息发送成功，如果 Exception 不为 null，说明消息发送失败。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/4d4dbf3f05b9b8980f5c1e74c4346b39.png" title="image-20220902161728576"><p>注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/f02ffeece212b5ebd04fb902d23bdfe0.png" title="image-20220902160941738"><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducer</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        <span class="comment">//服务信息</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;47.106.86.64:9092&quot;</span>);</span><br><span class="line">        <span class="comment">//配置序列化</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br><span class="line">        <span class="comment">// 2. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String,String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;one&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == <span class="literal">null</span>) &#123;</span><br><span class="line">                        System.out.println( <span class="string">&quot;分区 ： &quot;</span> + recordMetadata.partition() + <span class="string">&quot; 主题： &quot;</span> + recordMetadata.topic() );</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="同步发送API"><a href="#同步发送API" class="headerlink" title="同步发送API"></a>同步发送API</h3><ol><li>先处理已经堆积在DQueue中的数据。</li><li>RecordAccumulator再处理外部数据。</li></ol><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/11b8f9ad434c8ea76a97b0afb7ead8aa.png" title="image-20220902162557763"><p>只需在异步发送的基础上，再调用一下 get()方法即可。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/3296888f8ddde158197076fd51858ff2.png" title="image-20220902162232873"><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomProducerSync</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException &#123;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 1. 给 kafka 配置对象添加配置信息：bootstrap.servers</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        <span class="comment">//服务信息</span></span><br><span class="line">        properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,<span class="string">&quot;47.106.86.64:9092&quot;</span>);</span><br><span class="line">        <span class="comment">//配置序列化</span></span><br><span class="line">        properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());</span><br><span class="line">        <span class="comment">// 2. 创建 kafka 生产者的配置对象</span></span><br><span class="line">        KafkaProducer&lt;String, String&gt; kafkaProducer = <span class="keyword">new</span> <span class="title class_">KafkaProducer</span>&lt;String,String&gt;(properties);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 3. 创建 kafka 生产者对象</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            kafkaProducer.send(<span class="keyword">new</span> <span class="title class_">ProducerRecord</span>(<span class="string">&quot;first&quot;</span>, <span class="string">&quot;one&quot;</span> + i), <span class="keyword">new</span> <span class="title class_">Callback</span>() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> &#123;</span><br><span class="line">                    <span class="keyword">if</span> (e == <span class="literal">null</span>) &#123;</span><br><span class="line">                        System.out.println( <span class="string">&quot;分区 ： &quot;</span> + recordMetadata.partition() + <span class="string">&quot; 主题： &quot;</span> + recordMetadata.topic() );</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;).get();</span><br><span class="line">            Thread.sleep(<span class="number">100</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        kafkaProducer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="生产者拦截器"><a href="#生产者拦截器" class="headerlink" title="生产者拦截器"></a>生产者拦截器</h2><p><strong>生产者拦截器 （ProducerInterceptor）</strong></p><p>拦截器接口一共有三个方法。三个方法内的实现如果抛出异常，会被ProducerInterceptors内部捕获，并不会抛到上层。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ProducerInterceptor</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Configurable</span> &#123;</span><br><span class="line">    ProducerRecord&lt;K, V&gt; <span class="title function_">onSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span>;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span>;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>onSend</code> 方法在消息分区之前，可以对消息进行一定的修改，比如给key添加前缀，甚至可以修改我们的topic，如果需要使用kafka实现延时队列高级应用，我们就可以通过拦截器对消息进行判断，并修改，暂时放入我们的延时主题中，等时间达到再放回普通主题队列。</p><p><code>onAcknowledgement</code>该方法是在我们服务端对sender线程进行消息确认，或消息发送失败后的一个回调。优先于我们send方法的callback回调。我们可以对发送情况做一个统计。但是该方法在我们的sender线程也就是唯一的IO线程执行，逻辑越少越好。</p><p><code>close</code>该方法可以在关闭拦截器时，进行一些资源的释放。</p><p>（1） 实现自定义拦截器</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> MyInterceptor <span class="keyword">implements</span> <span class="title class_">ProducerInterceptor</span> &#123;</span><br><span class="line">    ProducerRecord&lt;K, V&gt; <span class="title function_">onSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span>;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span>;</span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（2）将自定义拦截器加入设置中</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG,MyInterceptor.getClass.getName());</span><br></pre></td></tr></table></figure><h2 id="生产者分区"><a href="#生产者分区" class="headerlink" title="生产者分区"></a>生产者分区</h2><h3 id="分区的好处"><a href="#分区的好处" class="headerlink" title="分区的好处"></a>分区的好处</h3><p>从存储的角度 -&gt; 合理使用存储资源，实现负载均衡</p><p>从计算的角度 -&gt; 提高并行计算的可行性</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/b2dbb52f914459787d686c44ebd7c3c8.png" title="image-20220902162836492"><h3 id="生产者发送消息分区策略"><a href="#生产者发送消息分区策略" class="headerlink" title="生产者发送消息分区策略"></a>生产者发送消息分区策略</h3><p>1）默认的分区器 DefaultPartitioner<br>在 IDEA 中 ctrl +n，全局查找 DefaultPartitioner。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/a37d58fcc2149fef557d391b76b98aff.png" title="image-20220902163515987"><p>Kafka支持三种分区策略 1) 指定分区； 2）指定key，计算hash得分区； 3）指定随机粘性分区；</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/18a0b6ba56db8e5b16b3d6fac9ba7fb7.png" title="image-20220902163808502"><h3 id="自定义分区器"><a href="#自定义分区器" class="headerlink" title="自定义分区器"></a>自定义分区器</h3><p>如果研发人员可以根据企业需求，自己重新实现分区器。</p><p>1）需求<br>例如我们实现一个分区器实现，发送过来的数据中如果包含 Hi，就发往 0 号分区，不包含 Hi，就发往 1 号分区。<br>2）实现步骤<br>（1）定义类实现 Partitioner 接口。<br>（2）重写 partition()方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyPartitioner</span> <span class="keyword">implements</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic 主题</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key 消息的 key</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyBytes 消息的 key 序列化后的字节数组</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value 消息的 value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueBytes 消息的 value 序列化后的字节数组</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster 集群元数据可以查看分区信息</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(String topic, Object key, <span class="type">byte</span>[] keyBytes, Object value, <span class="type">byte</span>[] valueBytes, Cluster cluster)</span> &#123;</span><br><span class="line">        <span class="type">String</span> <span class="variable">string</span> <span class="operator">=</span> value.toString();</span><br><span class="line">        <span class="keyword">if</span> (string.contains(<span class="string">&quot;vi&quot;</span>))&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（3）使用分区器的方法，在生产者的配置中添加分区器参数。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//自定义分区规则 </span></span><br><span class="line">properties.put(ProducerConfig.PARTITIONER_CLASS_CONFIG,MyPartitioner.class.getName());</span><br></pre></td></tr></table></figure><p>（4）开启测试</p><h2 id="生产者提高吞吐量"><a href="#生产者提高吞吐量" class="headerlink" title="生产者提高吞吐量"></a>生产者提高吞吐量</h2><p>通过提高吞吐量达到低延迟的效果</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/e29f428d28ae322918a211d256dd8b5b.png" title="image-20220902164746226"><p><strong>Batch.size 与 linger.ms 配合使用，根据生成数据的大小指定。</strong></p><p><strong>RecordAccumlator：在异步发送并且分区很多的情况下，32M的数据量容易被满足，进程交互加大，可以适当提高到64M。</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// batch.size：批次大小，默认 16K</span></span><br><span class="line">properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br><span class="line"><span class="comment">// linger.ms：等待时间，默认 0</span></span><br><span class="line">properties.put(ProducerConfig.LINGER_MS_CONFIG, <span class="number">1</span>);</span><br><span class="line"><span class="comment">// RecordAccumulator：缓冲区大小，默认 32M：buffer.memory</span></span><br><span class="line">properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG,<span class="number">33554432</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// compression.type：压缩，默认 none，可配置值 gzip、snappy、lz4 和 zstd</span></span><br><span class="line">properties.put(ProducerConfig.COMPRESSION_TYPE_CONFIG, <span class="string">&quot;snappy&quot;</span>);</span><br></pre></td></tr></table></figure><h3 id="消息累加器"><a href="#消息累加器" class="headerlink" title="消息累加器"></a>消息累加器</h3><p><strong>消息累加器（RecordAccumulator）</strong></p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/f20e0b29414b41978048b0c263c2f6a4.png" title="image-20220902174419992"><p>为了提高生产者的吞吐量，我们通过累加器将多条消息合并成一批统一发送。在broker中将消息批量存入。减少多次的网络IO。</p><p>消息累加器默认32m，如果生产者的发送速率大于sender发送的速率，消息就会堆满累加器。生产者就会阻塞，或者报错，报错取决于阻塞时间的配置。</p><p>累加器的存储形式为<code>ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt;</code>，可以看出来就是一个分区对应一个双端队列，队列中存储的是<code>ProducerBatch</code>一般大小是16k根据batch.size配置，新的消息会append到<code>ProducerBatch</code>中，满16k就会创建新的<code>ProducerBatch</code>，并且触发sender线程进行发送。</p><p>如果消息量非常大，生成了大量的<code>ProducerBatch</code>，在发送后，又需要JVM通过GC回收这些<code>ProducerBatch</code>就变得非常影响性能，所以kafka通过 <code>BufferPool</code>作为内存池来管理<code>ProducerBatch</code>的创建和回收，需要申请一个新的<code>ProducerBatch</code>空间时，调用 <code>free.allocate(size, maxTimeToBlock)</code>找内存池申请空间。</p><p>如果单条消息大于16k，那么就不会复用内存池了，会生成一个更大的<code>ProducerBatch</code>专门存放大消息，发送完后GC回收该内存空间。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">为了进一步减小网络中消息传输的带宽。我们也可以通过**消息压缩**的方式，在生产端将消息追加进`ProducerBatch`就对每一条消息进行压缩了。常用的有Gzip、Snappy、Lz4 和 Zstd，这是时间换空间的手段。压缩的消息会在消费端进行解压。</span><br></pre></td></tr></table></figure><h3 id="消息发送线程（Sender）"><a href="#消息发送线程（Sender）" class="headerlink" title="消息发送线程（Sender）"></a><strong>消息发送线程（Sender）</strong></h3><p>消息保存在内存后，Sender线程就会把符合条件的消息按照批次进行发送。除了发送消息，元数据的加载也是通过Sender线程来处理的。</p><p>Sender线程发送消息以及接收消息，都是基于java NIO的Selector。通过Selector把消息发出去，并通过Selector接收消息。</p><p>Sender线程默认容纳5个未确认的消息，消息发送失败后会进行重试。</p><h2 id="生产经验—数据可靠性"><a href="#生产经验—数据可靠性" class="headerlink" title="生产经验—数据可靠性"></a>生产经验—数据可靠性</h2><h3 id="消息确认机制-ACK"><a href="#消息确认机制-ACK" class="headerlink" title="消息确认机制-ACK"></a>消息确认机制-ACK</h3><p>producer提供了三种消息确认的模式，通过配置<code>acks</code>来实现</p><p><code>acks为0</code>时， 表示生产者将数据发送出去就不管了，不等待任何返回。这种情况下数据传输效率最高，但是数据可靠性最低，当 server挂掉的时候就会丢数据；</p><p><code>acks为1</code>时（默认），表示数据发送到Kafka后，经过leader成功接收消息的的确认，才算发送成功，如果leader宕机了，就会丢失数据。</p><p><code>acks为-1/all</code>时，表示生产者需要等待ISR中的所有follower都确认接收到数据后才算发送完成，这样数据不会丢失，因此可靠性最高，性能最低。</p><ul><li>数据完全可靠条件 &#x3D; ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量大于等于2</li></ul><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/a5c1a40450861d56bfc5cce44746a8a6.png" title="image-20220902172535966"><p>AR &#x3D; ISR + ORS</p><p>正常情况下，如果所有的follower副本都应该与leader副本保持一定程度的同步，则AR &#x3D; ISR，OSR &#x3D; null。</p><p>ISR 表示在指定时间内和leader保存数据同步的集合；</p><p>ORS表示不能在指定的时间内和leader保持数据同步集合，称为OSR(Out-Sync Relipca set)。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Ack 设置，默认是1</span></span><br><span class="line">properties.put(ProducerConfig.ACKS_CONFIG,<span class="string">&quot;1&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 重试次数, 默认的重试次数是 Max.Integer</span></span><br><span class="line">properties.put(ProducerConfig.RETRIES_CONFIG,<span class="number">3</span>);</span><br></pre></td></tr></table></figure><h3 id="数据去重-幂等性"><a href="#数据去重-幂等性" class="headerlink" title="数据去重-幂等性"></a>数据去重-幂等性</h3><p>1）幂等性原理</p><p>在一般的MQ模型中，常有以下的消息通信概念</p><ul><li><strong>至少一次（At Least Once）：</strong> ACK级别设置为-1 + 分区副本大于等于2 + ISR里应答的最小副本数量&gt;&#x3D;2。可以保证数据不丢失，但是不能保证数据不重复。</li><li><strong>最多一次（At Most Once）</strong>：ACK级别设置为0 。可以保证数据不重复，但是不能保证数据不丢失。•</li><li><strong>精确一次（Exactly Once）</strong>：至少一次 + 幂等性 。 Kafka 0.11版本引入一项重大特性：<strong>幂等性和事务</strong>。</li></ul><p>幂等性，简单地说就是对接口的多次调用所产生的结果和调用一次是一致的。生产者在进行重试的时候有可能会重复写入消息，而使用Kafka 的幂等性功能之后就可以避免这种情况。（不产生重复数据）</p><p><strong>重复数据的判断标准</strong>：具有&lt;PID, Partition, SeqNumber&gt;相同主键的消息提交时，Broker只会持久化一条。其</p><p>中ProducerId（pid）是Kafka每次重启都会分配一个新的；Partition 表示分区号；Sequence Number 序列化号，是单调自增的。</p><p>broker中会在内存维护一个pid+分区对应的序列号。如果收到的序列号正好比内存序列号大一，才存储消息，如果小于内存序列号，意味着消息重复，那么会丢弃消息，并应答。如果远大于内存序列号，意味着消息丢失，会抛出异常。</p><p>所以幂等解决的是sender到broker间，由于网络波动可能造成的重发问题。用幂等来标识唯一消息。</p><p>并且幂等性只能保证的是在单分区单会话内不重复。</p><p>2）如何使用幂等性</p><p>开启幂等性功能的方式很简单，只需要显式地将生产者客户端参数<code>enable.idempotence</code>设置为true即可(这个参数的默认值为true)，并且还需要确保生产者客户端的<strong>retries、acks、max.in.filght.request.per.connection</strong>参数不被配置错，默认值就是对的。</p><h3 id="消息事务"><a href="#消息事务" class="headerlink" title="消息事务"></a>消息事务</h3><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/4d210e935a7af0d15c3caa53e08f4e9e.png" title="image-20220902183826203"><p>由于幂等性不能跨分区运作，为了保证同时发的多条消息，要么全成功，要么全失败。kafka引入了事务的概念。</p><p>开启事务需要producer设置<code>transactional.id</code>的值并同时开启幂等性。</p><p>通过事务协调器，来实现事务，工作流程如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1 初始化事务</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">initTransactions</span><span class="params">()</span>;</span><br><span class="line"><span class="comment">// 2 开启事务</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">beginTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException;</span><br><span class="line"><span class="comment">// 3 在事务内提交已经消费的偏移量（主要用于消费者）</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">sendOffsetsToTransaction</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span><br><span class="line"><span class="params"> String consumerGroupId)</span> <span class="keyword">throws</span> </span><br><span class="line">ProducerFencedException;</span><br><span class="line"><span class="comment">// 4 提交事务</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">commitTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException;</span><br><span class="line"><span class="comment">// 5 放弃事务（类似于回滚事务的操作）</span></span><br><span class="line"><span class="keyword">void</span> <span class="title function_">abortTransaction</span><span class="params">()</span> <span class="keyword">throws</span> ProducerFencedException;</span><br></pre></td></tr></table></figure><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/95a605d47e3a21cfbc211419c3a2b0ab.png" title="image-20220902185538447"><h2 id="消息顺序"><a href="#消息顺序" class="headerlink" title="消息顺序"></a>消息顺序</h2><p>消息在单分区内有序，多分区内无序（如果对多分区进行排序，造成分区无法工作需要等待排序，浪费性能）</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/378373559c2bb13de4d456bde9a4f27a.png" title="image-20220902184011079"><p>kafka只能保证<code>单分区下的消息顺序性</code>，为了保证消息的顺序性，需要做到如下几点。</p><p><code>如果未开启幂等性</code>，需要 max.in.flight.requests.per.connection 设置为1。（缓冲队列最多放置1个请求）</p><p><code>如果开启幂等性</code>，需要 max.in.flight.requests.per.connection 设置为小于5。</p><p>这是因为broker端会缓存producer主题分区下的五个request，保证最近5个request是有序的。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/a3b4872eb57bd687ecfead941459546c.png" title="image-20220902184310223"><p>如果Request3在失败重试后才发往到集群中，必然会导致乱序，但是集群会重新按照序列号进行排序（最对一次排序5个）。</p><h1 id="Kafka-Broker"><a href="#Kafka-Broker" class="headerlink" title="Kafka Broker"></a>Kafka Broker</h1><h2 id="Broker设计"><a href="#Broker设计" class="headerlink" title="Broker设计"></a>Broker设计</h2><p>我们都知道kafka能堆积非常大的数据，一台服务器，肯定是放不下的。由此出现的集群的概念，集群不仅可以让消息负载均衡，还能提高消息存取的吞吐量。kafka集群中，会有多台broker，每台broker分别在不同的机器上。</p><p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-TKCzpF2O-1662356150180)(<a target="_blank" rel="noopener external nofollow noreferrer" href="http://mk-images.tagao.top/img/640?imageslim)%5D">http://mk-images.tagao.top/img/640?imageslim)]</a></p><p>为了提高吞吐量，每个topic也会都多个分区，同时为了保持可靠性，每个分区还会有多个副本。这些分区副本被均匀的散落在每个broker上，其中每个分区副本中有一个副本为leader，其他的为follower。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/37643c8b56fe0f32a3b9d7c803f85b0b.png" title="image-20220902195939625"><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><h3 id="Zookeeper作用"><a href="#Zookeeper作用" class="headerlink" title="Zookeeper作用"></a>Zookeeper作用</h3><p>Zookeeper在Kafka中扮演了重要的角色，kafka使用zookeeper进行元数据管理，保存broker注册信息，包括主题（Topic）、分区（Partition）信息等，选择分区leader。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/b1f0ebc535b00384be6bf81540c5f416.png" title="image-20220902200249692"><h3 id="Broker选举Leader"><a href="#Broker选举Leader" class="headerlink" title="Broker选举Leader"></a>Broker选举Leader</h3><p>这里需要先明确一个概念leader选举，因为kafka中涉及多处选举机制，容易搞混，Kafka由三个方面会涉及到选举：</p><ul><li>broker（控制器）选leader</li><li>分区多副本选leader</li><li>消费者选Leader</li></ul><p>在kafka集群中由很多的broker（也叫做控制器），但是他们之间需要选举出一个leader，其他的都是follower。broker的leader有很重要的作用，诸如：创建、删除主题、增加分区并分配leader分区；集群broker管理，包括新增、关闭和故障处理；分区重分配（auto.leader.rebalance.enable&#x3D;true，后面会介绍），分区leader选举。</p><p>每个broker都有唯一的brokerId，他们在启动后会去竞争注册zookeeper上的Controller结点，谁先抢到，谁就是broker leader。而其他broker会监听该结点事件，以便后续leader下线后触发重新选举。</p><p>简图：</p><ul><li>broker（控制器）选leader</li></ul><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/12b4f076e8f82c66b00ec8782433649f.png" title="image-20220902200901222"><p>详细图：</p><ul><li>broker（控制器）选leader</li><li>分区多副本选leader</li></ul><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/e5c14b9c17123faf3eefa58a22ab0668.png" title="image-20220902201352868"><p><strong>模拟 Kafka 上下线，Zookeeper 中数据变化</strong></p><p>（1）查看&#x2F;kafka&#x2F;brokers&#x2F;ids 路径上的节点。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 2] ls /kafka/brokers/ids</span><br><span class="line">[0, 1, 2]</span><br></pre></td></tr></table></figure><p>（2）查看&#x2F;kafka&#x2F;controller 路径上的数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 15] get /kafka/controller</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;brokerid&quot;:0,&quot;timestamp&quot;:&quot;1637292471777&quot;&#125;</span><br></pre></td></tr></table></figure><p>（3）查看&#x2F;kafka&#x2F;brokers&#x2F;topics&#x2F;first&#x2F;partitions&#x2F;0&#x2F;state 路径上的数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 16] get  /kafka/brokers/topics/first/partitions/0/state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:24,&quot;leader&quot;:0,&quot;version&quot;:1,&quot;leader_epoch&quot;:18,&quot; isr&quot;:[0,1,2]&#125;</span><br></pre></td></tr></table></figure><p>（4）停止 hadoop104 上的 kafka。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-stop.sh</span><br></pre></td></tr></table></figure><p>（5）再次查看&#x2F;kafka&#x2F;brokers&#x2F;ids 路径上的节点。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 3] ls /kafka/brokers/ids</span><br><span class="line">[0, 1]</span><br></pre></td></tr></table></figure><p>（6）再次查看&#x2F;kafka&#x2F;controller 路径上的数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 15] get /kafka/controller</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;brokerid&quot;:0,&quot;timestamp&quot;:&quot;1637292471777&quot;&#125;</span><br></pre></td></tr></table></figure><p>（7）再次查看&#x2F;kafka&#x2F;brokers&#x2F;topics&#x2F;first&#x2F;partitions&#x2F;0&#x2F;state 路径上的数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 16] get  /kafka/brokers/topics/first/partitions/0/state</span><br><span class="line">&#123;&quot;controller_epoch&quot;:24,&quot;leader&quot;:0,&quot;version&quot;:1,&quot;leader_epoch&quot;:18,&quot; isr&quot;:[0,1]&#125;</span><br></pre></td></tr></table></figure><p>（8）启动 hadoop104 上的 kafka。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-start.sh -daemon ./config/server.properties</span><br></pre></td></tr></table></figure><p>（9）再次观察（1）、（2）、（3）步骤中的内容。</p><h3 id="Broker重要参数"><a href="#Broker重要参数" class="headerlink" title="Broker重要参数"></a>Broker重要参数</h3><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/3313f521671b4d9bcc6a02d30e6e2920.png" title="image-20220902203152370"> <img src="/img/loading.gif" data-lazy-src="/posts/f351760e/bc280b8c039123a6d33d66be93eb26ac.png" title="image-20220902203207882"><h2 id="节点服役和退役"><a href="#节点服役和退役" class="headerlink" title="节点服役和退役"></a>节点服役和退役</h2><h3 id="服役新节点"><a href="#服役新节点" class="headerlink" title="服役新节点"></a>服役新节点</h3><p>(1) 启动一台新的KafKa服务端（加入原有的Zookeeper集群）</p><p>(2) 查看原有的 分区信息 describe</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --topic first --describe</span></span><br><span class="line"></span><br><span class="line">Topic: first	TopicId: 4DtkHPe4R1KyXNF7QyVqBA	PartitionCount: 3	ReplicationFactor: 3	Configs: segment.bytes=1073741824</span><br><span class="line">	Topic: first	Partition: 0	Leader: 1	Replicas: 2,1,0	Isr: 1,0</span><br><span class="line">	Topic: first	Partition: 1	Leader: 0	Replicas: 0,1,2	Isr: 0,1</span><br><span class="line">	Topic: first	Partition: 2	Leader: 1	Replicas: 1,2,0	Isr: 1,0</span><br></pre></td></tr></table></figure><p>(3) 指定需要均衡的主题</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">vim topics-to-move.json</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line"> &quot;topics&quot;: [</span><br><span class="line"> &#123;&quot;topic&quot;: &quot;first&quot;&#125;</span><br><span class="line"> ],</span><br><span class="line"> &quot;version&quot;: 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>(4) 生成负载均衡计划(只是生成计划)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-reassign-partitions.sh --bootstrap-server 47.106.86.64:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2,3&quot; --generate</span><br><span class="line"></span><br><span class="line">Current partition replica assignment</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replic</span><br><span class="line">as&quot;:[0,2,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;par</span><br><span class="line">tition&quot;:1,&quot;replicas&quot;:[2,1,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;to</span><br><span class="line">pic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,0,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;</span><br><span class="line">any&quot;,&quot;any&quot;]&#125;]&#125;</span><br><span class="line">Proposed partition reassignment configuration</span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replic</span><br><span class="line">as&quot;:[2,3,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;par</span><br><span class="line">tition&quot;:1,&quot;replicas&quot;:[3,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;to</span><br><span class="line">pic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;</span><br><span class="line">any&quot;,&quot;any&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure><p>（3）创建副本存储计划（所有副本存储在 broker0、broker1、broker2、broker3 中）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim increase-replication-factor.json</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[&#123;&quot;topic&quot;:&quot;first&quot;,&quot;partition&quot;:0,&quot;replic</span><br><span class="line">as&quot;:[2,3,0],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;topic&quot;:&quot;first&quot;,&quot;par</span><br><span class="line">tition&quot;:1,&quot;replicas&quot;:[3,0,1],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;any&quot;,&quot;any&quot;]&#125;,&#123;&quot;to</span><br><span class="line">pic&quot;:&quot;first&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,1,2],&quot;log_dirs&quot;:[&quot;any&quot;,&quot;</span><br><span class="line">any&quot;,&quot;any&quot;]&#125;]&#125;</span><br></pre></td></tr></table></figure><p>(5) 执行副本计划</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions.sh --bootstrap-server 47.106.86.64:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br></pre></td></tr></table></figure><p>(6) 验证计划</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions.sh --bootstrap-server 47.106.86.64:9092 --reassignment-json-file increase-replication-factor.json --verify</span><br><span class="line"></span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition first-0 is complete.</span><br><span class="line">Reassignment of partition first-1 is complete.</span><br><span class="line">Reassignment of partition first-2 is complete.</span><br><span class="line">Clearing broker-level throttles on brokers 0,1,2,3</span><br><span class="line">Clearing topic-level throttles on topic first</span><br></pre></td></tr></table></figure><h3 id="退役旧节点"><a href="#退役旧节点" class="headerlink" title="退役旧节点"></a>退役旧节点</h3><p><strong>1）执行负载均衡操作</strong><br>先按照退役一台节点，生成执行计划，然后按照服役时操作流程执行负载均衡。</p><p>不同于服役计划的 <code>--broker-list &quot;0,1,2&quot;</code> 退役了 Broker3 ；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions.sh --bootstrap-server 47.106.86.64:9092 --topics-to-move-json-file topics-to-move.json --broker-list &quot;0,1,2&quot; --generate</span><br></pre></td></tr></table></figure><h2 id="副本机制"><a href="#副本机制" class="headerlink" title="副本机制"></a>副本机制</h2><h3 id="副本基本信息"><a href="#副本基本信息" class="headerlink" title="副本基本信息"></a>副本基本信息</h3><ul><li><strong>Replica</strong> ：副本，同一分区的不同副本保存的是相同的消息，为保证集群中的某个节点发生故障时，该节点上的 partition <strong>数据不丢失 ，提高副本可靠性</strong>，且 kafka 仍然能够继续工作，kafka 提供了副本机制，一个 topic 的每个分区都有若干个副本，一个 leader 和若干个 follower。</li><li><strong>Leader</strong> ：每个分区的多个副本中的&quot;主副本&quot;，生产者以及消费者<strong>只与 Leader 交互</strong>。</li><li><strong>Follower</strong> ：每个分区的多个副本中的&quot;从副本&quot;，负责实时从 Leader 中同步数据，保持和 Leader 数据的同步。Leader 发生故障时，从 Follower 副本中重新选举新的 Leader 副本对外提供服务。</li></ul><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/1c5cd091fee1ef3c76fcc1c13dccf7de.png" title="image-20220902201925254"><ul><li><strong>AR</strong>:分区中的所有 Replica 统称为 AR &#x3D; ISR +OSR</li><li><strong>ISR</strong>:所有与 Leader 副本保持一定程度同步的Replica(包括 Leader 副本在内)组成 ISR</li><li><strong>OSR</strong>:与 Leader 副本同步滞后过多的 Replica 组成了 OSR</li><li><strong>LEO</strong>:每个副本都有内部的LEO，代表当前队列消息的最后一条偏移量offset + 1。</li><li><strong>HW</strong>:高水位，代表所有ISR中的LEO最低的那个offset，也是消费者可见的最大消息offset。</li></ul><h3 id="副本选举Leader"><a href="#副本选举Leader" class="headerlink" title="副本选举Leader"></a>副本选举Leader</h3><p>Kafka 集群中有一个 broker 的 Controller 会被选举为 Controller Leader (4.2.2) ，负责管理集群Broker 的上下线，所有 topic 的分区副本分配和 Leader 选举等工作。</p><p>Broker中Controller 的信息同步工作是依赖于 Zookeeper 的 .&#x2F;broker&#x2F;topic 目录下的信息。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/f8a5692cec80d597c164b8aea6a9b135.png" title="image-20220902205616764"><p><strong>结论先行： 如果leader副本下线， 会在ISR队列中存活为前提，按照Replicas队列中前面优先的原则。</strong></p><p>↓↓↓</p><p>（1）创建一个新的 topic，4 个分区，4 个副本</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --create --topic atguigu1 --partitions 4 --replication-factor 4</span><br></pre></td></tr></table></figure><p>（2）查看 Leader 分布情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --describe --topic atguigu1</span><br><span class="line"></span><br><span class="line">Topic: atguigu1 TopicId: awpgX_7WR-OX3Vl6HE8sVg PartitionCount: 4 ReplicationFactor: 4</span><br><span class="line">Configs: segment.bytes=1073741824</span><br><span class="line">Topic: atguigu1 Partition: 0 Leader: 3 Replicas: 3,0,2,1 Isr: 3,0,2,1</span><br><span class="line">Topic: atguigu1 Partition: 1 Leader: 1 Replicas: 1,2,3,0 Isr: 1,2,3,0</span><br><span class="line">Topic: atguigu1 Partition: 2 Leader: 0 Replicas: 0,3,1,2 Isr: 0,3,1,2</span><br><span class="line">Topic: atguigu1 Partition: 3 Leader: 2 Replicas: 2,1,0,3 Isr: 2,1,0,3</span><br></pre></td></tr></table></figure><p>（3）停止掉 hadoop105 的 kafka 进程，并查看 Leader 分区情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-stop.sh</span><br><span class="line"></span><br><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --describe --topic atguigu1</span><br><span class="line"></span><br><span class="line">Topic: atguigu1 TopicId: awpgX_7WR-OX3Vl6HE8sVg PartitionCount: 4 ReplicationFactor: 4</span><br><span class="line">Configs: segment.bytes=1073741824</span><br><span class="line">Topic: atguigu1 Partition: 0 Leader: 0 Replicas: 3,0,2,1 Isr: 0,2,1</span><br><span class="line">Topic: atguigu1 Partition: 1 Leader: 1 Replicas: 1,2,3,0 Isr: 1,2,0</span><br><span class="line">Topic: atguigu1 Partition: 2 Leader: 0 Replicas: 0,3,1,2 Isr: 0,1,2</span><br><span class="line">Topic: atguigu1 Partition: 3 Leader: 2 Replicas: 2,1,0,3 Isr: 2,1,0</span><br></pre></td></tr></table></figure><p>（4）停止掉 hadoop104 的 kafka 进程，并查看 Leader 分区情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-stop.sh</span><br><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --describe  --topic atguigu1</span><br><span class="line"></span><br><span class="line">Topic: atguigu1 TopicId: awpgX_7WR-OX3Vl6HE8sVg PartitionCount: 4 ReplicationFactor: 4</span><br><span class="line">Configs: segment.bytes=1073741824</span><br><span class="line">Topic: atguigu1 Partition: 0 Leader: 0 Replicas: 3,0,2,1 Isr: 0,1</span><br><span class="line">Topic: atguigu1 Partition: 1 Leader: 1 Replicas: 1,2,3,0 Isr: 1,0</span><br><span class="line">Topic: atguigu1 Partition: 2 Leader: 0 Replicas: 0,3,1,2 Isr: 0,1</span><br><span class="line">Topic: atguigu1 Partition: 3 Leader: 1 Replicas: 2,1,0,3 Isr: 1,0</span><br></pre></td></tr></table></figure><h3 id="副本故障处理"><a href="#副本故障处理" class="headerlink" title="副本故障处理"></a>副本故障处理</h3><h4 id="follower故障流程"><a href="#follower故障流程" class="headerlink" title="follower故障流程"></a><strong>follower故障流程</strong></h4><p>如果follower落后leader过多，体现在<strong>落后时间</strong> repca.lag.time.max.ms ，或者<strong>落后偏移量</strong>repca.lag.max.messages(由于kafka生成速度不好界定，后面取消了该参数)，follower就会被移除ISR队列，等待该队列LEO追上HW，才会重新加入ISR中。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/b01dad78f006b40d82e719fe71caeb78.png" title="image-20220902210759125"><h4 id="leader故障流程"><a href="#leader故障流程" class="headerlink" title="leader故障流程"></a><strong>leader故障流程</strong></h4><p>旧Leader先被从ISR队列中踢出，然后从ISR中选出一个新的Leader来；此时为了保证多个副本之间的数据一致性，其他的follower会先将各自的log文件中<strong>高于HW的部分截取掉</strong>，然后从新的leader同步数据（由此可知这只能保证副本之间数据一致性，并不能保证数据不丢失或者不重复）。体现了设置ACK-all的重要性。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/1f96e55810be5ff8cbf64c04b5d37315.png" title="image-20220902210830344"><h3 id="分区副本分配"><a href="#分区副本分配" class="headerlink" title="分区副本分配"></a>分区副本分配</h3><p>如果 kafka 服务器只有 4 个节点，那么设置 kafka 的分区数大于服务器台数，在 kafka底层如何分配存储副本呢？<br>1）创建 16 分区，3 个副本<br>（1）创建一个新的 topic，名称为 second。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --create --partitions 16 --replication-factor 3 --topic second</span><br></pre></td></tr></table></figure><p>（2）查看分区和副本情况。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server 47.106.86.64:9092  --describe --topic second</span><br><span class="line"></span><br><span class="line">Topic: second4 Partition: 0 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2</span><br><span class="line">Topic: second4 Partition: 1 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3</span><br><span class="line">Topic: second4 Partition: 2 Leader: 2 Replicas: 2,3,0 Isr: 2,3,0</span><br><span class="line">Topic: second4 Partition: 3 Leader: 3 Replicas: 3,0,1 Isr: 3,0,1</span><br><span class="line">Topic: second4 Partition: 4 Leader: 0 Replicas: 0,2,3 Isr: 0,2,3</span><br><span class="line">Topic: second4 Partition: 5 Leader: 1 Replicas: 1,3,0 Isr: 1,3,0</span><br><span class="line">Topic: second4 Partition: 6 Leader: 2 Replicas: 2,0,1 Isr: 2,0,1</span><br><span class="line">Topic: second4 Partition: 7 Leader: 3 Replicas: 3,1,2 Isr: 3,1,2</span><br><span class="line">Topic: second4 Partition: 8 Leader: 0 Replicas: 0,3,1 Isr: 0,3,1</span><br><span class="line">Topic: second4 Partition: 9 Leader: 1 Replicas: 1,0,2 Isr: 1,0,2</span><br><span class="line">Topic: second4 Partition: 10 Leader: 2 Replicas: 2,1,3 Isr: 2,1,3</span><br><span class="line">Topic: second4 Partition: 11 Leader: 3 Replicas: 3,2,0 Isr: 3,2,0</span><br><span class="line">Topic: second4 Partition: 12 Leader: 0 Replicas: 0,1,2 Isr: 0,1,2</span><br><span class="line">Topic: second4 Partition: 13 Leader: 1 Replicas: 1,2,3 Isr: 1,2,3</span><br><span class="line">Topic: second4 Partition: 14 Leader: 2 Replicas: 2,3,0 Isr: 2,3,0</span><br><span class="line">Topic: second4 Partition: 15 Leader: 3 Replicas: 3,0,1 Isr: 3,0,1</span><br></pre></td></tr></table></figure><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/15514da0f22aca9e3017fa2305733ba4.png" title="image-20220902211334365"><h3 id="手动调整分区副本"><a href="#手动调整分区副本" class="headerlink" title="手动调整分区副本"></a>手动调整分区副本</h3><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/b4b0afba1f2d4f558a6b36a57b20aa0e.png" title="image-20220902211501656"><p>手动调整分区副本存储的步骤如下：<br>（1）创建一个新的 topic，名称为 three。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server  47.106.86.64:9092  --create --partitions 4 --replication-factor 2 --topic three</span><br></pre></td></tr></table></figure><p>（3）创建副本存储计划（所有副本都指定存储在 broker0、broker1 中）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">vim increase-replication-factor.json</span></span><br><span class="line">输入如下内容：</span><br><span class="line">&#123;</span><br><span class="line">&quot;version&quot;:1,</span><br><span class="line">&quot;partitions&quot;:[</span><br><span class="line">&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,1]&#125;,</span><br><span class="line">&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1]&#125;,</span><br><span class="line">&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[1,0]&#125;,</span><br><span class="line">&#123;&quot;topic&quot;:&quot;three&quot;,&quot;partition&quot;:3,&quot;replicas&quot;:[1,0]&#125;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>（4）执行副本存储计划。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions.sh --bootstrap-server  47.106.86.64:9092  --reassignment-json-file increase-replication-factor.json --execute</span><br></pre></td></tr></table></figure><p>（5）验证副本存储计划。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions.sh --bootstrap-server  47.106.86.64:9092  --reassignment-json-file increase-replication-factor.json --verify</span><br></pre></td></tr></table></figure><h3 id="分区自动调整"><a href="#分区自动调整" class="headerlink" title="分区自动调整"></a><strong>分区自动调整</strong></h3><p>一般情况下，我们的分区都是平衡散落在broker的，随着一些broker故障，会慢慢出现leader集中在某台broker上的情况，造成集群负载不均衡，这时候就需要分区平衡。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/6a7af1df3bfe1be9186ac95ee3bee67d.png" title="image-20220902212238315"><p>为了解决上述问题kafka出现了自动平衡的机制。kafka提供了下面几个参数进行控制：</p><ul><li><code>auto.leader.rebalance.enable</code>：自动leader parition平衡，默认是true;</li><li><code>leader.imbalance.per.broker.percentage</code>：每个broker允许的不平衡的leader的比率，默认是10%，如果超过这个值，控制器将会触发leader的平衡</li><li><code>leader.imbalance.check.interval.seconds</code>：检查leader负载是否平衡的时间间隔，默认是300秒</li><li>但是在生产环境中是不开启这个自动平衡，因为触发leader partition的自动平衡会损耗性能，或者可以将触发自动平和的参数<code>leader.imbalance.per.broker.percentage</code>的值调大点。</li></ul><p>我们也可以通过修改配置，然后手动触发分区的再平衡。</p><h3 id="增加副本因子"><a href="#增加副本因子" class="headerlink" title="增加副本因子"></a>增加副本因子</h3><p>在生产环境当中，由于某个主题的重要等级需要提升，我们考虑增加副本。副本数的增加需要先制定计划，然后根据计划执行。<br>不能通过命令行的方法添加副本。<br>1）创建 topic</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --bootstrap-server 47.106.86.64:9092 --create --partitions 3 --replication-factor 1 --topic four</span><br></pre></td></tr></table></figure><p>2）手动增加副本存储<br>（1）创建副本存储计划（所有副本都指定存储在 broker0、broker1、broker2 中）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim increase-replication-factor.json</span><br><span class="line"></span><br><span class="line">&#123;&quot;version&quot;:1,&quot;partitions&quot;:[</span><br><span class="line">&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:0,&quot;replicas&quot;:[0,1,2]&#125;,</span><br><span class="line">&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:1,&quot;replicas&quot;:[0,1,2]&#125;,</span><br><span class="line">&#123;&quot;topic&quot;:&quot;four&quot;,&quot;partition&quot;:2,&quot;replicas&quot;:[0,1,2]&#125;]&#125;</span><br></pre></td></tr></table></figure><p>（2）执行副本存储计划。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-reassign-partitions.sh --bootstrap-server 47.106.86.64:9092 --reassignment-json-file increase-replication-factor.json --execute</span><br></pre></td></tr></table></figure><h2 id="文件存储"><a href="#文件存储" class="headerlink" title="文件存储"></a>文件存储</h2><h3 id="存储结构"><a href="#存储结构" class="headerlink" title="存储结构"></a><strong>存储结构</strong></h3><p>在Kafka中主题（Topic）是一个逻辑上的概念，分区（partition）是物理上的存在的。每个partition对应一个log文件，该log文件中存储的就是Producer生产的数据。Producer生产的数据会被不断追加到该log文件末端。为防止log文件过大导致数据定位效率低下，Kafka采用了分片和索引机制，将每个partition分为多个segment，每个segment默认1G（ <code>log.segment.bytes</code> ）， 每个segment包括.<strong>index</strong>文件、**.log<strong>文件和</strong>.timeindex**等文件。这些文件位于文件夹下，该文件命名规则为：topic名称+分区号。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/53141d358a80165c63374044d1bced37.png" title="image-20220902213237928"><p>Segment的三个文件需要通过特定工具打开才能看到信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kafka-run-class.sh kafka.tools.DumpLogSegments --files ./00000000000000000000.index</span><br><span class="line"> </span><br><span class="line">kafka-run-class.sh kafka.tools.DumpLogSegments --files ./00000000000000000000.log</span><br></pre></td></tr></table></figure><p>当log文件写入4k（这里可以通过<code>log.index.interval.bytes</code>设置）数据，就会写入一条索引信息到index文件中，这样的index索引文件就是一个<strong>稀疏索引</strong>，它并不会每条日志都建立索引信息。</p><p>当Kafka查询一条offset对应实际消息时，可以通过index进行二分查找，获取最近的低位offset，然后从低位offset对应的position开始，从实际的log文件中开始往后查找对应的消息。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/1fab494365192264be6926087c22b042.png" title="image-20220902213631039"><p><code>时间戳索引文件</code>，它的作用是可以查询某一个时间段内的消息，它的数据结构是：时间戳（8byte）+ 相对offset（4byte），如果要使用这个索引文件，先要通过时间范围找到对应的offset，然后再去找对应的index文件找到position信息，最后在遍历log文件，这个过程也是需要用到index索引文件的。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/6131b946d3ce07c2a1b5640854a43b53.png" title="image-20220902213600127"><h3 id="文件清理策略"><a href="#文件清理策略" class="headerlink" title="****文件清理策略"></a>****文件清理策略</h3><p>Kafka将消息存储在磁盘中，为了控制磁盘占用空间的不断增加就需要对消息做一定的清理操作。Kafka 中每一个分区副本都对应一个Log，而Log又可以分为多个日志分段，这样也便于日志的清理操作。Kafka提供了两种日志清理策略。</p><ol><li>日志删除(delete) :按照一定的保留策略直接删除不符合条件的日志分段。</li><li>日志压缩(compact) :针对每个消息的key进行整合，对于有相同key的不同value值，只保留最后一个版本。</li></ol><p>我们可以通过修改broker端参数 <code>log.cleanup.policy</code> 来进行配置</p><h4 id="日志删除"><a href="#日志删除" class="headerlink" title="日志删除"></a>日志删除</h4><p>kafka中默认的日志保存时间为7天，可以通过调整如下参数修改保存时间。</p><ul><li><code>log.retention.hours</code>：最低优先级小时，默认7天</li><li><code>log.retention.minutes</code>：分钟</li><li><code>log.retention.ms</code>：最高优先级毫秒</li><li><code>log.retention.check.interval.ms</code>：负责设置检查周期，默认5分钟</li><li><code>file.delete.delay.ms</code>：延迟执行删除时间</li><li><code>log.retention.bytes</code>：当设置为-1时表示运行保留日志最大值（相当于关闭）；当设置为1G时，表示日志文件最大值</li></ul><p>具体的保留日志策略有三种：</p><p><strong>基于时间策略</strong></p><p>日志删除任务会周期检查当前日志文件中是否有保留时间超过设定的阈值来寻找可删除的日志段文件集合；这里需要注意log.retention参数的优先级：<code>log.retention.ms &gt; log.retention.minutes &gt; log.retention.hours</code>，默认只会配置log.retention.hours参数，值为168即为<strong>7</strong>天。</p><p>删除过期的日志段文件，并不是简单的根据日志段文件的修改时间计算，而是要根据该日志段中最大的时间戳来计算的，首先要查询该日志分段所对应的时间戳索引文件，查找该时间戳索引文件的最后一条索引数据，如果时间戳大于0就取值，否则才会使用最近修改时间。</p><p>在删除的时候先从Log对象所维护的日志段的跳跃表中移除要删除的日志段，用来确保已经没有线程来读取这些日志段；接着将日志段所对应的所有文件，包括索引文件都添加上**.deleted<strong>的后缀；最后交给一个以</strong>delete-file<strong>命名的延迟任务来删除这些以</strong>.deleted<strong>为后缀的文件，默认是1分钟执行一次，可以通过</strong>file.delete.delay.ms**来配置。</p><p><strong>基于日志大小策略</strong></p><p>日志删除任务会周期性检查当前日志大小是否超过设定的阈值（log.retention.bytes，默认是-1，表示无穷大），就从第一个日志分段中寻找可删除的日志段文件集合。如果超过阈值，</p><p><strong>基于日志起始偏移量</strong></p><p>该策略判断依据是日志段的下一个日志段的起始偏移量 baseOffset是否小于等于 logStartOffset，如果是，则可以删除此日志分段。这里说一下logStartOffset，一般情况下，日志文件的起始偏移量 logStartOffset等于第一个日志分段的 baseOffset，但这并不是绝对的，logStartOffset的值可以通过 <strong>DeleteRecordsRequest</strong>请求、使用 <strong>kafka-delete-records.sh 脚本、日志的清理和截断</strong>等操作进行修改。</p><h4 id="日志压缩"><a href="#日志压缩" class="headerlink" title="日志压缩"></a>日志压缩</h4><p>日志压缩对于有相同key的不同value值，只保留最后一个版本。如果应用只关心 key对应的最新 value值，则可以开启 Kafka相应的日志清理功能，Kafka会定期将相同 key的消息进行合并，只保留最新的 value值。</p><ul><li>log.cleanup.policy &#x3D; compact 所有数据启用压缩策略</li></ul><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/33be0a0544a0cadb68ebc81b5d408dce.png" title="image-20220902214504699"><p>这种策略只适合特殊场景，比如消息的key是用户ID，value是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。</p><h2 id="Kafka高效读数据"><a href="#Kafka高效读数据" class="headerlink" title="Kafka高效读数据"></a><strong>Kafka高效读数据</strong></h2><p>kafka之所以可以快速读写的原因如下：</p><ol><li>kafka是分布式集群，采用分区方式，并行操作</li><li>读取数据采用稀疏索引，可以快速定位消费数据</li><li>顺序写磁盘</li><li>页缓冲和零拷贝</li></ol><h3 id="顺序写磁盘"><a href="#顺序写磁盘" class="headerlink" title="顺序写磁盘"></a>顺序写磁盘</h3><p>Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。<strong>官网有数据表明</strong>，同样的磁盘，顺序写能到 600M&#x2F;s，而随机写只有 100K&#x2F;s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/eeb8531c34ed091d57d9be10590839c5.png" title="image-20220902214803709"><h3 id="页缓存与零拷贝"><a href="#页缓存与零拷贝" class="headerlink" title="页缓存与零拷贝"></a>页缓存与零拷贝</h3><p>kafka高效读写的原因很大一部分取决于<strong>页缓存</strong>和<strong>零拷贝</strong></p><h4 id="页缓存"><a href="#页缓存" class="headerlink" title="页缓存"></a><strong>页缓存</strong></h4><p>在 Kafka 中，大量使用了 <code>PageCache</code>， 这也是 Kafka 能实现高吞吐的重要因素之一。</p><p>首先看一下读操作，当一个进程要去读取磁盘上的文件内容时，操作系统会先查看要读取的数据页是否缓冲在<code>PageCache</code> 中，如果存在则直接返回要读取的数据，这就减少了对于磁盘 I&#x2F;O的 操作；但是如果没有查到，操作系统会向磁盘发起读取请求并将读取的数据页存入 <code>PageCache</code> 中，之后再将数据返回给进程，就和使用redis缓冲是一个道理。</p><p>接着写操作和读操作是一样的，如果一个进程需要将数据写入磁盘，操作系统会检查数据页是否在PageCache 中已经存在，如果不存在就在 PageCache中添加相应的数据页，接着将数据写入对应的数据页。另外被修改过后的数据页也就变成了脏页，操作系统会在适当时间将脏页中的数据写入磁盘，以保持数据的一致性。</p><p>具体的刷盘机制可以通过 <code>log.flush.interval messages</code>，<code>log.flush .interval .ms</code> 等参数来控制。同步刷盘可以提高消息的可靠性，防止由于机器 掉电等异常造成处于页缓存而没有及时写入磁盘的消息丢失。一般并不建议这么做，刷盘任务就应<strong>交由操作系统去调配</strong>，消息的可靠性应该由多副本机制来保障，而不是由同步刷盘这 种严重影响性能的行为来保障 。</p><h4 id="零拷贝"><a href="#零拷贝" class="headerlink" title="零拷贝"></a><strong>零拷贝</strong></h4><p>零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数，通常使用在IO读写过程中。常规应用程序IO过程如下图，会经过四次拷贝：</p><ol><li>数据从磁盘经过DMA(直接存储器访问)到内核的Read Buffer；</li><li>内核态的Read Buffer到用户态应用层的Buffer</li><li>用户态的Buffer到内核态的Socket Buffer</li><li>Socket Buffer到网卡的NIC Buffer</li></ol><p>从上面的流程可以知道内核态和用户态之间的拷贝相当于执行两次无用的操作，之间切换也会花费很多资源；当数据从磁盘经过DMA 拷贝到内核缓存（页缓存）后，为了减少CPU拷贝的性能损耗，操作系统会将该内核缓存与用户层进行共享，减少一次CPU copy过程，同时用户层的读写也会直接访问该共享存储，本身由用户层到Socket缓存的数据拷贝过程也变成了从内核到内核的CPU拷贝过程，更加的快速，这就是零拷贝，IO流程如下图。</p><p>甚至如果我们的消息存在页缓存<code>PageCache</code>中，还避免了硬盘到内核的拷贝过程，更加一步提升了消息的吞吐量。 (大概就理解成传输的数据只保存在内核空间，不需要再拷贝到用户态的应用层)</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/a023c56d8c21509914b60e5338a733af.png" title="image-20220902214906599"><hr><p>Java的JDK NIO中方法transferTo()方法就能够实现零拷贝操作，这个实现依赖于操作系统底层的sendFile()实现的</p><h1 id="Kafka消费者"><a href="#Kafka消费者" class="headerlink" title="Kafka消费者"></a>Kafka消费者</h1><h2 id="消费模式"><a href="#消费模式" class="headerlink" title="消费模式"></a><strong>消费模式</strong></h2><p>常见的消费模式有两种：</p><p><code>poll(拉)</code>：消费者主动向服务端拉取消息。</p><p><code>push(推)</code>：服务端主动推送消息给消费者。</p><p>由于推模式很难考虑到每个客户端不同的消费速率,导致消费者无法消费消息而宕机，因此kafka采用的是poll的模式，该模式有个缺点，如果服务端没有消息，消费端就会一直空轮询。为了避免过多不必要的空轮询，kafka做了改进，如果没消息服务端就会暂时保持该请求，在一段时间内有消息再回应给客户端。</p><h2 id="消费工作流程"><a href="#消费工作流程" class="headerlink" title="消费工作流程"></a>消费工作流程</h2><h3 id="消费者总体工作流程"><a href="#消费者总体工作流程" class="headerlink" title="消费者总体工作流程"></a>消费者总体工作流程</h3><p>消费者对消息进行消费，并且将已经消费的消息加入 _consumer_offsets 中。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/bf2f3ba0edfdc7e956cb2dfb9969f7d6.png" title="image-20220903110040620"><h3 id="消费者组原理"><a href="#消费者组原理" class="headerlink" title="消费者组原理"></a><strong>消费者组原理</strong></h3><p>Consumer Group（CG）：消费者组，由多个consumer组成。形成一个消费者组的条件，是所有消费者的groupid相同。</p><ul><li>消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费。</li><li>消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。</li></ul><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/cd28478848b1e8a07862df32d89d186f.png" title="image-20220903103656937"><p>对于消息中间件而言，一般有两种消息投递模式：<strong>点对点</strong>(P2P, Point-to-Point)模式和<strong>发布／订阅</strong>(Pub&#x2F;Sub)模式。点对点模式是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。发布订阅模式定义了如何向一个内容节点发布和订阅消息，这个内容节点称为主题(Topic) , 主题可以认为是消息传递的中介，消息发布者将消息发布到某个主题， 而消息订阅者从主题中订阅消息。主题使得消息的订阅者和发布者互相保持独立，不需要进行 接触即可保证消息的传递，发布／订阅模式在消息的一对多广播时采用。Kafka同时支待两种消息投递模式，而这正是得益于消费者与消费组模型的契合：</p><ul><li>如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。</li><li>如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布／订阅模式的应用。</li></ul><h3 id="消费者组选举Leader"><a href="#消费者组选举Leader" class="headerlink" title="消费者组选举Leader"></a>消费者组选举Leader</h3><p>具体的消费者组初始化流程：</p><p>通过对GroupId进行Hash得到那台服务器的coordinator ，coordinator负责选出消费组中的Leader ，并且协调信息。真正存储消费记录的是 _consumer_offsets_partition 。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/04af8e9bf63614d3036c551bb0cd6bee.png" title="image-20220903103841994"> <img src="/img/loading.gif" data-lazy-src="/posts/f351760e/51e900538b008741cbb654c47ae767d2.png" title="image-20220903112720568"> <img src="/img/loading.gif" data-lazy-src="/posts/f351760e/5f505f4ff893cff095dad1a816ee357d.png" title="image-20220903112925137"> <img src="/img/loading.gif" data-lazy-src="/posts/f351760e/8752d547e23158aa7f8d4576bca220d5.png" title="image-20220903112936658"><h2 id="消费者API"><a href="#消费者API" class="headerlink" title="消费者API"></a>消费者API</h2><p>消费组单消费者以及消费者组多消费者</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/5c501aa681b86866001bf6716331246d.png" title="image-20220903113854959"><p>注意：在消费者 API 代码中必须配置消费者组 id。命令行启动消费者不填写消费者组id 会被自动填写随机的消费者组 id。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">CustomConsumer</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        <span class="comment">//0.配置信息</span></span><br><span class="line">        <span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">        properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;47.106.86.64:9092&quot;</span>);</span><br><span class="line">        properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        properties.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">&quot;test&quot;</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//1.创建消费者</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; kafkaConsumer = <span class="keyword">new</span> <span class="title class_">KafkaConsumer</span>&lt;&gt;(properties);</span><br><span class="line">        ArrayList&lt;String&gt; topic = <span class="keyword">new</span> <span class="title class_">ArrayList</span>&lt;&gt;();</span><br><span class="line">        topic.add(<span class="string">&quot;first&quot;</span>);</span><br><span class="line">        kafkaConsumer.subscribe(topic);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//2.消费信息</span></span><br><span class="line">        <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">            records.forEach(record -&gt; &#123;</span><br><span class="line">                System.out.println(record);</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//3.关闭</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="分区平衡以及再平衡"><a href="#分区平衡以及再平衡" class="headerlink" title="分区平衡以及再平衡"></a>分区平衡以及再平衡</h2><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/33482e5e97cb53dcaf744c3da25794be.png" title="image-20220903114711746"><p>参数名称 描述<br><code>heartbeat.interval.ms</code> Kafka 消费者和 coordinator 之间的心跳时间，默认 3s。该条目的值必须小于 session.timeout.ms，也不应该高于session.timeout.ms 的 1&#x2F;3。</p><p><code>session.timeout.ms</code> Kafka 消费者和 coordinator 之间连接超时时间，默认 45s。超过该值，该消费者被移除，消费者组执行再平衡。</p><p><code>max.poll.interval.ms</code> 消费者处理消息的最大时长，默认是 5 分钟。超过该值，该消费者被移除，消费者组执行再平衡。</p><p><code>partition.assignment.strategy</code> 消 费 者 分 区 分 配 策 略 ， 默 认 策 略 是 Range + CooperativeSticky。Kafka 可以同时使用多个分区分配策略。可 以 选 择 的 策 略 包 括 ： Range 、 RoundRobin 、 Sticky 、CooperativeSticky (协作者粘性)</p><h3 id="分区分配策略"><a href="#分区分配策略" class="headerlink" title="分区分配策略"></a><strong>分区分配策略</strong></h3><p>我们知道一个 Consumer Group 中有多个 Consumer，一个 Topic 也有多个 Partition，所以必然会涉及到 Partition 的分配问题: 确定哪个 Partition 由哪个 Consumer 来消费的问题。</p><p>Kafka 客户端提供了3 种分区分配策略：<strong>RangeAssignor</strong>、<strong>RoundRobinAssignor</strong> 和 <strong>StickyAssignor</strong>，前两种 分配方案相对简单一些StickyAssignor分配方案相对复杂一些。</p><h3 id="Range"><a href="#Range" class="headerlink" title="Range"></a><strong>Range</strong></h3><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/a2ae2716aaa6c40e6192344eb4039ea2.png" title="image-20220903104029564"><p>Range 分区分配再平衡案例<br>（1）停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。<br>1 号消费者：消费到 3、4 号分区数据。<br>2 号消费者：消费到 5、6 号分区数据。<br>0 号消费者的任务会整体被分配到 1 号消费者或者 2 号消费者。 (被整体分配)<br>说明：0 号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p><p>（2）再次重新发送消息观看结果（45s 以后）。<br>1 号消费者：消费到 0、1、2、3 号分区数据。<br>2 号消费者：消费到 4、5、6 号分区数据。<br>说明：消费者 0 已经被踢出消费者组，所以重新按照 range 方式分配。</p><h3 id="RoundRobin"><a href="#RoundRobin" class="headerlink" title="RoundRobin"></a><strong>RoundRobin</strong></h3><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/26d3849b324e10e6356382d88877af11.png" title="image-20220903104000261"><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改分区分配策略</span></span><br><span class="line">properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, <span class="string">&quot;org.apache.kafka.clients.consumer.RoundRobinAssignor&quot;</span>);</span><br></pre></td></tr></table></figure><p>（1）停止掉 0 号消费者，快速重新发送消息观看结果（45s 以内，越快越好）。<br>1 号消费者：消费到 2、5 号分区数据<br>2 号消费者：消费到 4、1 号分区数据<br>0 号消费者的任务会按照 RoundRobin 的方式，把数据轮询分成 0 、6 和 3 号分区数据，分别由 1 号消费者或者 2 号消费者消费。（采用轮训）<br>说明：0 号消费者挂掉后，消费者组需要按照超时时间 45s 来判断它是否退出，所以需要等待，时间到了 45s 后，判断它真的退出就会把任务分配给其他 broker 执行。</p><p>（2）再次重新发送消息观看结果（45s 以后）。<br>1 号消费者：消费到 0、2、4、6 号分区数据<br>2 号消费者：消费到 1、3、5 号分区数据<br>说明：消费者 0 已经被踢出消费者组，所以重新按照 RoundRobin 方式分配。</p><h3 id="Sticky："><a href="#Sticky：" class="headerlink" title="Sticky："></a><strong>Sticky：</strong></h3><p><strong>StickyAssignor</strong> 分区分配算法是 Kafka 客户端提供的分配策略中最复杂的一种，可以通过 partition.assignment.strategy 参数去设置，从 0.11 版本开始引入，目的就是在执行新分配时，尽量在上一次分配结果上少做调整，其主要实现了以下2个目标：</p><p>1)、<strong>Topic Partition</strong> 的分配要尽量均衡。</p><p>2)、当 <strong>Rebalance</strong>(重分配，后面会详细分析) 发生时，尽量与上一次分配结果保持一致。</p><p>该算法的精髓在于，重分配后，还能尽量与上一次结果保持一致，进而达到消费者故障下线，故障恢复后的均衡问题，在此就不举例了。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 修改分区分配策略</span></span><br><span class="line">properties.put(ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG, <span class="string">&quot;org.apache.kafka.clients.consumer.StickyAssignor&quot;</span>);</span><br></pre></td></tr></table></figure><h2 id="offset位移提交"><a href="#offset位移提交" class="headerlink" title="offset位移提交"></a><strong>offset位移提交</strong></h2><h3 id="offset-的默认维护位置"><a href="#offset-的默认维护位置" class="headerlink" title="offset 的默认维护位置"></a>offset 的默认维护位置</h3><p>Kafka 0.9 版本之前consumer默认将offset保存在Zookeeper中，从0.9版本之后consumer默认保存在Kafka一个内置的topic中，该topic为_consumer_offsets。</p><p>消费者提交的offset值维护在**__consumer_offsets<strong>这个Topic中，具体维护在哪个分区中，是由消费者所在的消费者组</strong>groupid**决定，计算方式是：groupid的hashCode值对50取余。当kafka环境正常而消费者不能消费时，有可能是对应的__consumer_offsets分区leader为none或-1，或者分区中的日志文件损坏导致。</p><p>__consumer_offsets 主题里面采用 key 和 value 的方式存储数据。key 是 group.id+topic+ 分区号，value 就是当前 offset 的值。每隔一段时间，kafka 内部会对这个 topic 进行 compact，也就是每个 group.id+topic+分区号就保留最新数据。</p><p>一般情况下， 当集群中第一次有消费者消费消息时会自动创建主题_ consumer_ offsets, 不过它的副本因子还受offsets.topic .replication.factor参数的约束，这个参数的默认值为3 (下载安装的包中此值可能为1)，分区数可以通过offsets.topic.num.partitions参数设置，默认为50。</p><p>在配置文件 config&#x2F;consumer.properties 中添加配置 exclude.internal.topics&#x3D;false，默认是 true，表示不能消费系统主题。为了查看该系统主题数据，所以该参数修改为 false。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh --topic __consumer_offsets --bootstrap-server 47.106.86.64:9092 --consumer.config config/consumer.properties --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageFormatter&quot; --from-beginning</span><br><span class="line">1</span><br><span class="line">[offset,atguigu,1]::OffsetAndMetadata(offset=7, </span><br><span class="line">leaderEpoch=Optional[0], metadata=, commitTimestamp=1622442520203, </span><br><span class="line">expireTimestamp=None)</span><br><span class="line">[offset,atguigu,0]::OffsetAndMetadata(offset=8, </span><br><span class="line">leaderEpoch=Optional[0], metadata=, commitTimestamp=1622442520203, </span><br><span class="line">expireTimestamp=None)</span><br></pre></td></tr></table></figure><p>消费者提交offset的方式有两种，<strong>自动提交</strong>和<strong>手动提交</strong></p><h3 id="自动提交"><a href="#自动提交" class="headerlink" title="自动提交"></a>自动提交</h3><p>为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/2ab1b519114fc0e895726f632ecd7d87.png" title="image-20220903122129755"><ul><li><code>enable.auto.commit</code>：是否开启自动提交offset功能，默认是true</li><li><code>auto.commit.interval.ms</code>：自动提交offset的时间间隔，默认是5s</li></ul><p>自动提交有可能出现消息消费失败，但是却提交了offset的情况，导致<strong>消息丢失</strong>。为了能够实现消息消费offset的精确控制，更推荐手动提交。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 自动提交</span></span><br><span class="line">properties.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="literal">true</span>);</span><br><span class="line"><span class="comment">// 提交时间间隔</span></span><br><span class="line">properties.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,<span class="number">1000</span>);</span><br></pre></td></tr></table></figure><h3 id="手动提交"><a href="#手动提交" class="headerlink" title="手动提交"></a>手动提交</h3><p>虽然自动提交offset十分简单便利，但由于其是基于时间提交的，开发人员难以把握offset提交的时机。因 此Kafka还提供了手动提交offset的API。手动提交offset的方法有两种：分别是commitSync（<strong>同步提交</strong>）和commitAsync（<strong>异步提交</strong>）。两者的相同点是，都会将<strong>本次提交的一批数据最高的偏移量提交</strong>；不同点是，<strong>同步提交阻塞当前线程</strong>，一直到提交成功，并且会自动失败重试（由不可控因素导致，也会出现提交失败）；而<strong>异步提交则没有失败重试机制，故有可能提交失败</strong>。</p><ul><li>commitSync（同步提交）：必须等待offset提交完毕，再去消费下一批数据。 阻塞线程，一直到提交到成功，会进行失败重试</li><li>commitAsync（异步提交） ：发送完提交offset请求后，就开始消费下一批数据了。没有失败重试机制，会提交失败</li></ul><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/d3f45411f09af44343af1387a32c9034.png" title="image-20220903122747576"><h3 id="指定消费位置"><a href="#指定消费位置" class="headerlink" title="指定消费位置"></a><strong>指定消费位置</strong></h3><p>在kafka中当消费者<strong>查找不到所记录的消费位移</strong>时，会根据auto.offset.reset的配置，决定从何处消费。</p><p><code>auto.offset.reset = earliest | latest | none</code> 默认是 latest。</p><ul><li><code>earliest</code>：不管你的offset，始终从最早的位置开始拉取。</li><li><code>latest</code>（默认值）：自动将偏移量重置为最新偏移量，从最后提交的offset的位置拉取。</li><li><code>none</code>：如果未找到消费者组的先前偏移量，则向消费者抛出异常。</li></ul><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/bcc93d66821e2f1ac22c74c3eb00d9a3.png" title="image-20220903123232954"><p>Kafka中的消费位移是存储在一个内部主题中的， 而我们可以使用**seek()**方法可以突破这一限制：消费位移可以保存在任意的存储介质中， 例如数据库、 文件系统等。以数据库为例， 我们将消费位移保存在其中的一个表中， 在下次消费的时候可以读取存储在数据表中的消费位移并通过seek()方法指向这个具体的位置 。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//配置信息</span></span><br><span class="line"><span class="type">Properties</span> <span class="variable">properties</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Properties</span>();</span><br><span class="line">properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">&quot;latest&quot;</span>);</span><br></pre></td></tr></table></figure><p><strong>指定位移消费</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定位置进行消费</span></span><br><span class="line">Set&lt;TopicPartition&gt; assignment = kafkaConsumer.assignment();</span><br><span class="line"></span><br><span class="line"><span class="comment">//  保证分区分配方案已经制定完毕</span></span><br><span class="line"><span class="keyword">while</span> (assignment.size() == <span class="number">0</span>)&#123;</span><br><span class="line">    kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line">    assignment = kafkaConsumer.assignment();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定消费的offset</span></span><br><span class="line"><span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">    kafkaConsumer.seek(topicPartition,<span class="number">600</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3  消费数据</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line"></span><br><span class="line">    ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line">        System.out.println(consumerRecord);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>指定时间消费</strong></p><p>原理就是查到时间对应的offset再去指定位移消费，为了确保同步到分区信息，我们还需要确保能获取到分区，再去查询分区时间</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定位置进行消费</span></span><br><span class="line">Set&lt;TopicPartition&gt; assignment = kafkaConsumer.assignment();</span><br><span class="line"></span><br><span class="line"><span class="comment">//  保证分区分配方案已经制定完毕</span></span><br><span class="line"><span class="keyword">while</span> (assignment.size() == <span class="number">0</span>)&#123;</span><br><span class="line">    kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    assignment = kafkaConsumer.assignment();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 希望把时间转换为对应的offset</span></span><br><span class="line">HashMap&lt;TopicPartition, Long&gt; topicPartitionLongHashMap = <span class="keyword">new</span> <span class="title class_">HashMap</span>&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 封装对应集合</span></span><br><span class="line"><span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line">    topicPartitionLongHashMap.put(topicPartition,System.currentTimeMillis() - <span class="number">1</span> * <span class="number">24</span> * <span class="number">3600</span> * <span class="number">1000</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Map&lt;TopicPartition, OffsetAndTimestamp&gt; topicPartitionOffsetAndTimestampMap = kafkaConsumer.offsetsForTimes(topicPartitionLongHashMap);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定消费的offset</span></span><br><span class="line"><span class="keyword">for</span> (TopicPartition topicPartition : assignment) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="type">OffsetAndTimestamp</span> <span class="variable">offsetAndTimestamp</span> <span class="operator">=</span> topicPartitionOffsetAndTimestampMap.get(topicPartition);</span><br><span class="line"></span><br><span class="line">    kafkaConsumer.seek(topicPartition,offsetAndTimestamp.offset());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 3  消费数据</span></span><br><span class="line"><span class="keyword">while</span> (<span class="literal">true</span>)&#123;</span><br><span class="line"></span><br><span class="line">    ConsumerRecords&lt;String, String&gt; consumerRecords = kafkaConsumer.poll(Duration.ofSeconds(<span class="number">1</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; consumerRecord : consumerRecords) &#123;</span><br><span class="line"></span><br><span class="line">        System.out.println(consumerRecord);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="漏消费和重复消费"><a href="#漏消费和重复消费" class="headerlink" title="漏消费和重复消费"></a>漏消费和重复消费</h3><p>重复消费：已经消费了数据，但是 offset 没提交。<br>漏消费：先提交 offset 后消费，有可能会造成数据的漏消费。</p><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/e9052c6bfd3e5507ccd65eea215b1df6.png" title="image-20220903123840273"><h2 id="消费者事务"><a href="#消费者事务" class="headerlink" title="消费者事务"></a><strong>消费者事务</strong></h2><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/00ebe6445117fa1089fcb5f935c142d3.png" title="image-20220903123934358"><h2 id="数据积压（提高吞吐量）"><a href="#数据积压（提高吞吐量）" class="headerlink" title="数据积压（提高吞吐量）"></a><strong>数据积压（提高吞吐量）</strong></h2><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/d2a6d5fea05e3b26f1a2c51f71bea99c.png" title="image-20220903124022229"><p>参数名称 描述<br><code>fetch.max.bytes</code> 默认Default: 52428800（50 m）。消费者获取服务器端一批消息最大的字节数。如果服务器端一批次的数据大于该值（50m）仍然可以拉取回来这批数据，因此，这不是一个绝对最大值。一批次的大小受 message.max.bytes （broker config）ormax.message.bytes （topic config）影响。</p><p><code>max.poll.records</code> 一次 poll 拉取数据返回消息的最大条数，默认是 500 条</p><h2 id="拦截器"><a href="#拦截器" class="headerlink" title="拦截器"></a><strong>拦截器</strong></h2><p>与生产者对应，消费者也有拦截器。我们来看看拦截器具体的方法。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title class_">ConsumerInterceptor</span>&lt;K, V&gt; <span class="keyword">extends</span> <span class="title class_">Configurable</span>, AutoCloseable &#123;</span><br><span class="line"></span><br><span class="line">    ConsumerRecords&lt;K, V&gt; <span class="title function_">onConsume</span><span class="params">(ConsumerRecords&lt;K, V&gt; records)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">onCommit</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets)</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Kafka Consumer会在<strong>poll()**方法返回之前调用拦截器的**onConsume()**方法来对消息进行相应的定制化操作，比如**修改返回的消息内容<strong>、</strong>按照某种规则过滤消息</strong>（可能会减少poll()方法返回 的消息的个数）。如果onConsume()方法中抛出异常， 那么会被捕获并记录到日志中， 但是异常不会再向上传递。</p><p>Kafka Consumer会在提交完消费位移之后调用拦截器的**onCommit()**方法， 可以使用这个方法来记录跟踪所提交的位移信息，比如当消费者使用commitSync的无参方法时，我们不知道提交的消费位移的具体细节， 而使用拦截器的onCommit()方法却可以做到这 一点。</p><h1 id="Kafka整合Spring-Boot"><a href="#Kafka整合Spring-Boot" class="headerlink" title="Kafka整合Spring Boot"></a>Kafka整合Spring Boot</h1><ol><li>导包 -除了Spring Boot 之外还需要额外导入 Spring Web、Kafka</li></ol><img src="/img/loading.gif" data-lazy-src="/posts/f351760e/f8dad88ad0584fdf0307aabffabe8713.png" title="image-20220903182633431"><p>2, 编写配置文件</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">kafka:</span></span><br><span class="line">    <span class="attr">bootstrap-servers:</span> <span class="number">47.106</span><span class="number">.86</span><span class="number">.64</span><span class="string">:9092</span></span><br><span class="line">    <span class="attr">consumer:</span></span><br><span class="line">      <span class="attr">group-id:</span> <span class="string">test2</span></span><br><span class="line">      <span class="comment"># #序列化器 以及反序列化器</span></span><br><span class="line">      <span class="attr">key-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">      <span class="attr">value-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">    <span class="attr">producer:</span></span><br><span class="line">      <span class="comment"># #序列化器 以及反序列化器</span></span><br><span class="line">      <span class="attr">key-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">      <span class="attr">value-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br></pre></td></tr></table></figure><p>3、定义简单生产者</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ProducerController</span> &#123;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    KafkaTemplate&lt;String, String&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@RequestMapping(&quot;/hi&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> String  <span class="title function_">data</span><span class="params">(String msg)</span>&#123;</span><br><span class="line">        kafkaTemplate.send(<span class="string">&quot;first&quot;</span>,msg);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;ok&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>编写具有回调函数的生产者</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@GetMapping(&quot;/kafka/callbackOne/&#123;message&#125;&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage2</span><span class="params">(<span class="meta">@PathVariable(&quot;message&quot;)</span> String callbackMessage)</span> &#123;</span><br><span class="line">    kafkaTemplate.send(<span class="string">&quot;first&quot;</span>, callbackMessage).addCallback(success -&gt; &#123;</span><br><span class="line">        <span class="comment">// 消息发送到的topic</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> success.getRecordMetadata().topic();</span><br><span class="line">        <span class="comment">// 消息发送到的分区</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">partition</span> <span class="operator">=</span> success.getRecordMetadata().partition();</span><br><span class="line">        <span class="comment">// 消息在分区内的offset</span></span><br><span class="line">        <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> success.getRecordMetadata().offset();</span><br><span class="line">        System.out.println(<span class="string">&quot;发送消息成功:&quot;</span> + topic + <span class="string">&quot;-&quot;</span> + partition + <span class="string">&quot;-&quot;</span> + offset);</span><br><span class="line">    &#125;, failure -&gt; &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;发送消息失败:&quot;</span> + failure.getMessage());</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@GetMapping(&quot;/kafka/callbackTwo/&#123;message&#125;&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage3</span><span class="params">(<span class="meta">@PathVariable(&quot;message&quot;)</span> String callbackMessage)</span> &#123;</span><br><span class="line">    kafkaTemplate.send(<span class="string">&quot;first&quot;</span>, callbackMessage).addCallback(</span><br><span class="line">        (ListenableFutureCallback&lt;? <span class="built_in">super</span> SendResult&lt;String, String&gt;&gt;) <span class="keyword">new</span> <span class="title class_">ListenableFutureCallback</span>&lt;SendResult&lt;String, Object&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onFailure</span><span class="params">(Throwable ex)</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;发送消息失败：&quot;</span>+ex.getMessage());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onSuccess</span><span class="params">(SendResult&lt;String, Object&gt; result)</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;发送消息成功：&quot;</span> + result.getRecordMetadata().topic() + <span class="string">&quot;-&quot;</span></span><br><span class="line">                                   + result.getRecordMetadata().partition() + <span class="string">&quot;-&quot;</span> + result.getRecordMetadata().offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>4、定义消费者 -&gt;启动监听线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConfiguration</span>&#123;</span><br><span class="line">    <span class="meta">@KafkaListener(topics = &#123;&quot;first&quot;&#125;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">message1</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span>&#123;</span><br><span class="line">        <span class="comment">// 消费的哪个topic、partition的消息,打印出消息内容</span></span><br><span class="line">        System.out.println(<span class="string">&quot;点对点消费1：&quot;</span>+record.topic()+<span class="string">&quot;-&quot;</span>+record.partition()+<span class="string">&quot;-&quot;</span>+record.value());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RequestMapping(&quot;/hi&quot;)</span></span><br><span class="line"><span class="keyword">public</span> String  <span class="title function_">data</span><span class="params">(String msg)</span>&#123;</span><br><span class="line">    kafkaTemplate.send(<span class="string">&quot;first&quot;</span>,msg);</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;ok&quot;</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">编写具有回调函数的生产者</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line"><span class="meta">@GetMapping(&quot;/kafka/callbackOne/&#123;message&#125;&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage2</span><span class="params">(<span class="meta">@PathVariable(&quot;message&quot;)</span> String callbackMessage)</span> &#123;</span><br><span class="line">    kafkaTemplate.send(<span class="string">&quot;first&quot;</span>, callbackMessage).addCallback(success -&gt; &#123;</span><br><span class="line">        <span class="comment">// 消息发送到的topic</span></span><br><span class="line">        <span class="type">String</span> <span class="variable">topic</span> <span class="operator">=</span> success.getRecordMetadata().topic();</span><br><span class="line">        <span class="comment">// 消息发送到的分区</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">partition</span> <span class="operator">=</span> success.getRecordMetadata().partition();</span><br><span class="line">        <span class="comment">// 消息在分区内的offset</span></span><br><span class="line">        <span class="type">long</span> <span class="variable">offset</span> <span class="operator">=</span> success.getRecordMetadata().offset();</span><br><span class="line">        System.out.println(<span class="string">&quot;发送消息成功:&quot;</span> + topic + <span class="string">&quot;-&quot;</span> + partition + <span class="string">&quot;-&quot;</span> + offset);</span><br><span class="line">    &#125;, failure -&gt; &#123;</span><br><span class="line">        System.out.println(<span class="string">&quot;发送消息失败:&quot;</span> + failure.getMessage());</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@GetMapping(&quot;/kafka/callbackTwo/&#123;message&#125;&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">sendMessage3</span><span class="params">(<span class="meta">@PathVariable(&quot;message&quot;)</span> String callbackMessage)</span> &#123;</span><br><span class="line">    kafkaTemplate.send(<span class="string">&quot;first&quot;</span>, callbackMessage).addCallback(</span><br><span class="line">        (ListenableFutureCallback&lt;? <span class="built_in">super</span> SendResult&lt;String, String&gt;&gt;) <span class="keyword">new</span> <span class="title class_">ListenableFutureCallback</span>&lt;SendResult&lt;String, Object&gt;&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onFailure</span><span class="params">(Throwable ex)</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;发送消息失败：&quot;</span>+ex.getMessage());</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">onSuccess</span><span class="params">(SendResult&lt;String, Object&gt; result)</span> &#123;</span><br><span class="line">                System.out.println(<span class="string">&quot;发送消息成功：&quot;</span> + result.getRecordMetadata().topic() + <span class="string">&quot;-&quot;</span></span><br><span class="line">                                   + result.getRecordMetadata().partition() + <span class="string">&quot;-&quot;</span> + result.getRecordMetadata().offset());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>4、定义消费者 -&gt;启动监听线程</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Configuration</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">KafkaConfiguration</span>&#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@KafkaListener(topics = &#123;&quot;first&quot;&#125;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">message1</span><span class="params">(ConsumerRecord&lt;?, ?&gt; record)</span>&#123;</span><br><span class="line">        <span class="comment">// 消费的哪个topic、partition的消息,打印出消息内容</span></span><br><span class="line">        System.out.println(<span class="string">&quot;点对点消费1：&quot;</span>+record.topic()+<span class="string">&quot;-&quot;</span>+record.partition()+<span class="string">&quot;-&quot;</span>+record.value());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>参考 ：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/prague6695/article/details/123869202">https://blog.csdn.net/prague6695/article/details/123869202</a></p><h1 id="Kafka-Consumer配置"><a href="#Kafka-Consumer配置" class="headerlink" title="Kafka Consumer配置"></a>Kafka Consumer配置</h1><p>在0.9.0.0中，我们引入了新的Java消费者来替代早期基于Scala的简单和高级消费者。新老客户端的配置如下。</p><h2 id="新消费者配置"><a href="#新消费者配置" class="headerlink" title="新消费者配置"></a>新消费者配置</h2><p>新消费者配置：（注意，右面是可拖动的）</p><table><thead><tr><th>NAME</th><th>DESCRIPTION</th><th>TYPE</th><th>DEFAULT</th><th>VALID VALUES</th><th>IMPORTANCE</th></tr></thead><tbody><tr><td>bootstrap.servers</td><td>host&#x2F;port,用于和kafka集群建立初始化连接。因为这些服务器地址仅用于初始化连接，并通过现有配置的来发现全部的kafka集群成员（集群随时会变化），所以此列表不需要包含完整的集群地址（但尽量多配置几个，以防止配置的服务器宕机）。</td><td>list</td><td></td><td></td><td>high</td></tr><tr><td>key.deserializer</td><td>key的解析序列化接口实现类（Deserializer）。</td><td>class</td><td></td><td></td><td>high</td></tr><tr><td>value.deserializer</td><td>value的解析序列化接口实现类（Deserializer）</td><td>class</td><td></td><td></td><td>high</td></tr><tr><td>fetch.min.bytes</td><td>服务器哦拉取请求返回的最小数据量，如果数据不足，请求将等待数据积累。默认设置为1字节，表示只要单个字节的数据可用或者读取等待请求超时，就会应答读取请求。将此值设置的越大将导致服务器等待数据累积的越长，这可能以一些额外延迟为代价提高服务器吞吐量。</td><td>int</td><td>1</td><td>[0,...]</td><td>high</td></tr><tr><td>group.id</td><td>此消费者所属消费者组的唯一标识。如果消费者用于订阅或offset管理策略的组管理功能，则此属性是必须的。</td><td>string</td><td>&quot;&quot;</td><td></td><td>high</td></tr><tr><td>heartbeat.interval.ms</td><td>当使用Kafka的分组管理功能时，心跳到消费者协调器之间的预计时间。心跳用于确保消费者的会话保持活动状态，并当有新消费者加入或离开组时方便重新平衡。该值必须必比session.timeout.ms小，通常不高于1&#x2F;3。它可以调整的更低，以控制正常重新平衡的预期时间。</td><td>int</td><td>3000(3秒)</td><td></td><td>high</td></tr><tr><td>max.partition.fetch.bytes</td><td>服务器将返回每个分区的最大数据量。如果拉取的第一个非空分区中第一个消息大于此限制，则仍然会返回消息，以确保消费者可以正常的工作。broker接受的最大消息大小通过<code>message.max.bytes</code>（broker config）或<code>max.message.bytes</code> (topic config)定义。参阅fetch.max.bytes以限制消费者请求大小。</td><td>int</td><td>1048576</td><td>[0,...]</td><td>high</td></tr><tr><td>session.timeout.ms</td><td>用于发现消费者故障的超时时间。消费者周期性的发送心跳到broker，表示其还活着。如果会话超时期满之前没有收到心跳，那么broker将从分组中移除消费者，并启动重新平衡。请注意，该值必须在broker配置的<code>group.min.session.timeout.ms</code>和<code>group.max.session.timeout.ms</code>允许的范围内。</td><td>int</td><td>45000(45s)</td><td></td><td>high</td></tr><tr><td>ssl.key.password</td><td>密钥存储文件中的私钥的密码。 客户端可选</td><td>password</td><td>null</td><td></td><td>high</td></tr><tr><td>ssl.keystore.location</td><td>密钥存储文件的位置， 这对于客户端是可选的，并且可以用于客户端的双向认证。</td><td>string</td><td>null</td><td></td><td>high</td></tr><tr><td>ssl.keystore.password</td><td>密钥仓库文件的仓库密码。客户端可选，只有ssl.keystore.location配置了才需要。</td><td>password</td><td>null</td><td></td><td>high</td></tr><tr><td>ssl.truststore.location</td><td>信任仓库文件的位置</td><td>string</td><td>null</td><td></td><td>high</td></tr><tr><td>ssl.truststore.password</td><td>信任仓库文件的密码</td><td>password</td><td>null</td><td></td><td>high</td></tr><tr><td>auto.offset.reset</td><td>当Kafka中没有初始offset或如果当前的offset不存在时（例如，该数据被删除了），该怎么办。 *earliest：自动将偏移重置为最早的偏移 * latest：自动将偏移重置为最新偏移 *none：如果消费者组找到之前的offset，则向消费者抛出异常 * 其他：抛出异常给消费者。</td><td>string</td><td>latest</td><td>[latest, earliest, none]</td><td>medium</td></tr><tr><td>connections.max.idle.ms</td><td>指定在多少毫秒之后关闭闲置的连接</td><td>long</td><td>540000</td><td></td><td>medium</td></tr><tr><td>enable.auto.commit</td><td>如果为true，消费者的offset将在后台周期性的提交</td><td>boolean</td><td>true</td><td></td><td>medium</td></tr><tr><td>exclude.internal.topics</td><td>内部topic的记录（如偏移量）是否应向消费者公开。如果设置为true，则从内部topic接受记录的唯一方法是订阅它。</td><td>boolean</td><td>true</td><td></td><td>medium</td></tr><tr><td>fetch.max.bytes</td><td>服务器为拉取请求返回的最大数据值。这不是绝对的最大值，如果在第一次非空分区拉取的第一条消息大于该值，该消息将仍然返回，以确保消费者继续工作。接收的最大消息大小通过message.max.bytes (broker config) 或 max.message.bytes (topic config)定义。注意，消费者是并行执行多个提取的。</td><td>int</td><td>52428800</td><td>[0,...]</td><td>medium</td></tr><tr><td>max.poll.interval.ms</td><td>使用消费者组管理时poll()调用之间的最大延迟。消费者在获取更多记录之前可以空闲的时间量的上限。如果此超时时间期满之前poll()没有调用，则消费者被视为失败，并且分组将重新平衡，以便将分区重新分配给别的成员。</td><td>int</td><td>300000</td><td>[1,...]</td><td>medium</td></tr><tr><td>max.poll.records</td><td>在单次调用<code>poll()</code>中返回的最大消息数。</td><td>int</td><td>500</td><td>[1,...]</td><td>medium</td></tr><tr><td>partition.assignment.strategy</td><td>当使用组管理时，客户端将使用分区分配策略的类名来分配消费者实例之间的分区所有权</td><td>list</td><td>class org.apache.kafka .clients.consumer .RangeAssignor</td><td></td><td>medium</td></tr><tr><td>receive.buffer.bytes</td><td>读取数据时使用的TCP接收缓冲区（SO_RCVBUF）的大小。 如果值为-1，则将使用OS默认值。</td><td>int</td><td>65536</td><td>[-1,...]</td><td>medium</td></tr><tr><td>request.timeout.ms</td><td>配置控制客户端等待请求响应的最长时间。 如果在超时之前未收到响应，客户端将在必要时重新发送请求，如果重试耗尽则客户端将重新发送请求。</td><td>int</td><td>305000</td><td>[0,...]</td><td>medium</td></tr><tr><td>sasl.jaas.config</td><td>JAAS配置文件中SASL连接登录上下文参数。 这里描述JAAS配置文件格式。 该值的格式为： &#39;(&#x3D;)*;&#39;</td><td>password</td><td>null</td><td></td><td>medium</td></tr><tr><td>sasl.kerberos.service.name</td><td>Kafka运行Kerberos principal名。可以在Kafka的JAAS配置文件或在Kafka的配置文件中定义。</td><td>string</td><td>null</td><td></td><td>medium</td></tr><tr><td>sasl.mechanism</td><td>用于客户端连接的SASL机制。安全提供者可用的机制。GSSAPI是默认机制。</td><td>string</td><td>GSSAPI</td><td></td><td>medium</td></tr><tr><td>security.protocol</td><td>用于与broker通讯的协议。 有效值为：PLAINTEXT，SSL，SASL_PLAINTEXT，SASL_SSL。</td><td>string</td><td>PLAINTEXT</td><td></td><td>medium</td></tr><tr><td>send.buffer.bytes</td><td>发送数据时要使用的TCP发送缓冲区（SO_SNDBUF）的大小。 如果值为-1，则将使用OS默认值。</td><td>int</td><td>131072</td><td>[-1,...]</td><td>medium</td></tr><tr><td>ssl.enabled.protocols</td><td>启用SSL连接的协议列表。</td><td>list</td><td>TLSv1.2,TLSv1.1,TLSv1</td><td></td><td>medium</td></tr><tr><td>ssl.keystore.type</td><td>key仓库文件的文件格式，客户端可选。</td><td>string</td><td>JKS</td><td></td><td>medium</td></tr><tr><td>ssl.protocol</td><td>用于生成SSLContext的SSL协议。 默认设置是TLS，这对大多数情况都是适用的。 最新的JVM中的允许值为TLS，TLSv1.1和TLSv1.2。 较旧的JVM可能支持SSL，SSLv2和SSLv3，但由于已知的安全漏洞，不建议使用SSL。</td><td>string</td><td>TLS</td><td></td><td>medium</td></tr><tr><td>ssl.provider</td><td>用于SSL连接的安全提供程序的名称。 默认值是JVM的默认安全提供程序。</td><td>string</td><td>null</td><td></td><td>medium</td></tr><tr><td>ssl.truststore.type</td><td>信任存储文件的文件格式。</td><td>string</td><td>JKS</td><td></td><td>medium</td></tr><tr><td>auto.commit.interval.ms</td><td>如果enable.auto.commit设置为true，则消费者偏移量自动提交给Kafka的频率（以毫秒为单位）。</td><td>int</td><td>5000</td><td>[0,...]</td><td>low</td></tr><tr><td>check.crcs</td><td>自动检查CRC32记录的消耗。 这样可以确保消息发生时不会在线或磁盘损坏。 此检查增加了一些开销，因此在寻求极致性能的情况下可能会被禁用。</td><td>boolean</td><td>true</td><td></td><td>low</td></tr><tr><td>client.id</td><td>在发出请求时传递给服务器的id字符串。 这样做的目的是通过允许将逻辑应用程序名称包含在服务器端请求日志记录中，来跟踪ip&#x2F;port的请求源。</td><td>string</td><td>&quot;&quot;</td><td></td><td>low</td></tr><tr><td>fetch.max.wait.ms</td><td>如果没有足够的数据满足fetch.min.bytes，服务器将在接收到提取请求之前阻止的最大时间。</td><td>int</td><td>500</td><td>[0,...]</td><td>low</td></tr><tr><td>interceptor.classes</td><td>用作拦截器的类的列表。 你可实现ConsumerInterceptor接口以允许拦截（也可能变化）消费者接收的消息。 默认情况下，没有拦截器。</td><td>list</td><td>null</td><td></td><td>low</td></tr><tr><td>metadata.max.age.ms</td><td>在一定时间段之后（以毫秒为单位的），强制更新元数据，即使没有任何分区领导变化，任何新的broker或分区。</td><td>long</td><td>300000</td><td>[0,...]</td><td>low</td></tr><tr><td>metric.reporters</td><td>用作度量记录员类的列表。实现MetricReporter接口以允许插入通知新的度量创建的类。JmxReporter始终包含在注册JMX统计信息中。</td><td>list</td><td>&quot;&quot;</td><td></td><td>low</td></tr><tr><td>metrics.num.samples</td><td>保持的样本数以计算度量。</td><td>int</td><td>2</td><td>[1,...]</td><td>low</td></tr><tr><td>metrics.recording.level</td><td>最高的记录级别。</td><td>string</td><td>INFO</td><td>[INFO, DEBUG]</td><td>low</td></tr><tr><td>metrics.sample.window.ms</td><td>The window of time a metrics sample is computed over.</td><td>long</td><td>30000</td><td>[0,...]</td><td>low</td></tr><tr><td>reconnect.backoff.ms</td><td>尝试重新连接指定主机之前等待的时间，避免频繁的连接主机，这种机制适用于消费者向broker发送的所有请求。</td><td>long</td><td>50</td><td>[0,...]</td><td>low</td></tr><tr><td>retry.backoff.ms</td><td>尝试重新发送失败的请求到指定topic分区之前的等待时间。避免在某些故障情况下，频繁的重复发送。</td><td>long</td><td>100</td><td>[0,...]</td><td>low</td></tr><tr><td>sasl.kerberos.kinit.cmd Kerberos</td><td>kinit命令路径。</td><td>string</td><td>&#x2F;usr&#x2F;bin&#x2F;kinit</td><td></td><td>low</td></tr><tr><td>sasl.kerberos.min.time.before.relogin</td><td>尝试&#x2F;恢复之间的登录线程的休眠时间。</td><td>long</td><td>60000</td><td></td><td>low</td></tr><tr><td>sasl.kerberos.ticket.renew.jitter</td><td>添加到更新时间的随机抖动百分比。</td><td>double</td><td>0.05</td><td></td><td>low</td></tr><tr><td>sasl.kerberos.ticket.renew.window.factor</td><td>登录线程将休眠，直到从上次刷新到ticket的指定的时间窗口因子到期，此时将尝试续订ticket。</td><td>double</td><td>0.8</td><td></td><td>low</td></tr><tr><td>ssl.cipher.suites</td><td>密码套件列表，用于TLS或SSL网络协议的安全设置，认证，加密，MAC和密钥交换算法的明明组合。默认情况下，支持所有可用的密码套件。</td><td>list</td><td>null</td><td></td><td>low</td></tr><tr><td>ssl.endpoint.identification.algorithm</td><td>使用服务器证书验证服务器主机名的端点识别算法。</td><td>string</td><td>null</td><td></td><td>low</td></tr><tr><td>ssl.keymanager.algorithm</td><td>密钥管理器工厂用于SSL连接的算法。 默认值是为Java虚拟机配置的密钥管理器工厂算法。</td><td>string</td><td>SunX509</td><td></td><td>low</td></tr><tr><td>ssl.secure.random.implementation</td><td>用于SSL加密操作的SecureRandom PRNG实现。</td><td>string</td><td>null</td><td></td><td>low</td></tr><tr><td>ssl.trustmanager.algorithm</td><td>信任管理器工厂用于SSL连接的算法。 默认值是为Java虚拟机配置的信任管理器工厂算法。</td><td>string</td><td>PKIX</td><td></td><td>low</td></tr></tbody></table><h2 id="kafka-2-0-0"><a href="#kafka-2-0-0" class="headerlink" title="kafka &gt;&#x3D; 2.0.0"></a>kafka &gt;&#x3D; 2.0.0</h2><table><thead><tr><th>名称</th><th>描述</th><th>类型</th><th>默认</th><th>有效值</th><th>重要程度</th></tr></thead><tbody><tr><td>sasl.client.callback.handler.class</td><td>实现AuthenticateCallbackHandler接口的SASL客户端回调处理程序类的全称。</td><td>class</td><td>null</td><td></td><td>中间</td></tr><tr><td>sasl.login.callback.handler.class</td><td>实现AuthenticateCallbackHandler接口的SASL登录回调处理程序类的全称。对于broker来说，登录回调处理程序配置必须以监听器前缀和小写的SASL机制名称为前缀。例如，<code>listener.name.sasl_ssl.scram-sha-256.sasl.login.callback.handler.class=com.example.CustomScramLoginCallbackHandler</code></td><td>class</td><td>null</td><td></td><td>中间</td></tr><tr><td>sasl.login.class</td><td>实现Login接口的类的全称。对于broker来说，login config必须以监听器前缀和SASL机制名称为前缀，并使用小写。例如，<code>listener.name.sasl_ssl.scram-sha-256.sasl.login.class=com.example.CustomScramLogin</code>。</td><td>class</td><td>null</td><td></td><td>中间</td></tr></tbody></table><h2 id="kafka-2-1-0"><a href="#kafka-2-1-0" class="headerlink" title="kafka &gt;&#x3D; 2.1.0"></a>kafka &gt;&#x3D; 2.1.0</h2><table><thead><tr><th>名称</th><th>描述</th><th>类型</th><th>默认</th><th>有效值</th><th>重要程度</th></tr></thead><tbody><tr><td>client.dns.lookup</td><td>控制客户端如何使用DNS查询。如果设置为 <code>use_all_dns_ips</code>，则依次连接到每个返回的IP地址，直到成功建立连接。断开连接后，使用下一个IP。一旦所有的IP都被使用过一次，客户端就会再次从主机名中解析IP(s)(然而，JVM和操作系统都会缓存DNS名称查询)。如果设置为 <code>resolve_canonical_bootstrap_servers_only</code>，则将每个引导地址解析成一个canonical名称列表。在bootstrap阶段之后，这和<code>use_all_dns_ips</code>的行为是一样的。如果设置为 <code>default</code>(已弃用)，则尝试连接到查找返回的第一个IP地址，即使查找返回多个IP地址。</td><td>string</td><td>use_all_dns_ips</td><td><code>[default, use_all_dns_ips, resolve_canonical_bootstrap_servers_only]</code></td><td>中间</td></tr></tbody></table><h2 id="kafka-2-7"><a href="#kafka-2-7" class="headerlink" title="kafka &gt;&#x3D; 2.7"></a>kafka &gt;&#x3D; 2.7</h2><table><thead><tr><th>名称</th><th>描述</th><th>类型</th><th>默认</th><th>有效值</th><th>重要程度</th></tr></thead><tbody><tr><td>ssl.truststore.certificates</td><td>可信证书的格式由&#39;ssl.truststore.type&#39;指定。默认的SSL引擎工厂只支持带X.509证书的PEM格式。</td><td>password</td><td>null</td><td></td><td>高</td></tr><tr><td>socket.connection.setup.timeout.max.ms</td><td>客户端等待建立socket连接的最大时间。连接设置超时时间将随着每一次连续的连接失败而成倍增加，直到这个最大值。为了避免连接风暴，超时时间将被应用一个0.2的随机因子，导致计算值在20%以下和20%以上的随机范围。</td><td>long</td><td>127000 (127 seconds)</td><td></td><td>中间</td></tr><tr><td>socket.connection.setup.timeout.ms</td><td>客户端等待建立socket连接的时间。如果在超时之前没有建立连接，客户端将关闭socket通道。</td><td>long</td><td>10000 (10 seconds)</td><td></td><td>中间</td></tr></tbody></table><h2 id="旧消费者配置"><a href="#旧消费者配置" class="headerlink" title="旧消费者配置"></a>旧消费者配置</h2><p>旧消费者配置如下：</p><ul><li>group.id</li><li>zookeeper.connect</li></ul><table><thead><tr><th>PROPERTY</th><th>DEFAULT</th><th>DESCRIPTION</th></tr></thead><tbody><tr><td>group.id</td><td></td><td>标识消费者所属消费者组(独一的)。通过设置相同的组ID，多个消费者表明属于该消费者组的一部分。</td></tr><tr><td>zookeeper.connect</td><td></td><td>指定ZooKeeper连接字符串，格式为hostname：port，其中host和port是ZooKeeper服务器的主机和端口。 为了使ZooKeeper宕机时连接到其他ZooKeeper节点，你还可以以hostname1:host1，hostname2:port2，hostname3:port3的形式指定多个主机。 还可以设置ZooKeeper chroot路径，作为其ZooKeeper连接字符串的一部分，将其数据放置在全局ZooKeeper命名空间中的某个路径下。 如果是这样，消费者应该在其连接字符串中使用相同的chroot路径。 例如，要给出&#x2F;chroot&#x2F;path的chroot路径，你需要将该值设置为：<code>hostname1:port1,hostname2:port2,hostname3:port3/chroot/path</code>。</td></tr><tr><td>consumer.id</td><td>null</td><td>如果未设置将自动生成。</td></tr><tr><td>socket.timeout.ms</td><td>30 * 1000</td><td>网络请求socker的超时时间。实际的超时是 max.fetch.wait+socket.timeout.ms的时间。</td></tr><tr><td>socket.receive.buffer.bytes</td><td>64 * 1024</td><td>网络请求socker的接收缓存大小</td></tr><tr><td>fetch.message.max.bytes</td><td>1024 * 1024</td><td>每个拉取请求的每个topic分区尝试获取的消息的字节大小。这些字节将被读入每个分区的内存，因此这有助于控制消费者使用的内存。 拉取请求的大小至少与服务器允许的最大消息的大小一样大，否则生产者可能发送大于消费者可以拉取的消息。</td></tr><tr><td>num.consumer.fetchers</td><td>1</td><td>用于拉取数据的拉取线程数。</td></tr><tr><td>auto.commit.enable</td><td>true</td><td>如果为true，请定期向ZooKeeper提交消费者已经获取的消息的偏移量。 当进程失败时，将使用这种承诺偏移量作为新消费者开始的位置。</td></tr><tr><td>auto.commit.interval.ms</td><td>60 * 1000</td><td>消费者offset提交到zookeeper的频率（以毫秒为单位）</td></tr><tr><td>queued.max.message.chunks</td><td>2</td><td>消费缓存消息块的最大大小。每个块可以达到fetch.message.max.bytes。</td></tr><tr><td>rebalance.max.retries</td><td>4</td><td>当新的消费者加入消费者组时，消费者集合尝试“重新平衡”负载，并为每个消费者分配分区。如果消费者集合在分配时发生时发生变化，则重新平衡将失败并重试。此设置控制尝试之前的最大尝试次数。</td></tr><tr><td>fetch.min.bytes</td><td>1</td><td>拉取请求返回最小的数据量。如果没有足够的数据，请求将等待数据积累，然后应答请求。</td></tr><tr><td>fetch.wait.max.ms</td><td>100</td><td>如果没有足够的数据（fetch.min.bytes），服务器将在返回请求数据之前阻塞的最长时间。</td></tr><tr><td>rebalance.backoff.ms</td><td>2000</td><td>重新平衡时重试之间的回退时间。如果未设置，则使用zookeeper.sync.time.ms中的值。</td></tr><tr><td>refresh.leader.backoff.ms</td><td>200</td><td>回退时间等待，然后再尝试选举一个刚刚失去leader的分区。</td></tr><tr><td>auto.offset.reset</td><td>largest</td><td>如果ZooKeeper中没有初始偏移量，或偏移值超出范围，该怎么办？ *最小：自动将偏移重置为最小偏移 * 最大：自动将偏移重置为最大偏移 * 其他任何事情：抛出异常消费者</td></tr><tr><td>consumer.timeout.ms</td><td>-1</td><td>如果在指定的时间间隔后没有消息可用，则向用户发出超时异常</td></tr><tr><td>exclude.internal.topics</td><td>true</td><td>来自内部topic的消息（如偏移量）是否应该暴露给消费者。</td></tr><tr><td>client.id</td><td>group id value</td><td>客户端ID是每个请求中发送的用户指定的字符串，用于帮助跟踪调用。 它应该逻辑地标识发出请求的应用程序。</td></tr><tr><td>zookeeper.session.timeout.ms</td><td>6000</td><td>ZooKeeper会话超时。如果消费者在这段时间内没有对ZooKeeper心跳，那么它被认为是死亡的，并且会发生重新平衡。</td></tr><tr><td>zookeeper.connection.timeout.ms</td><td>6000</td><td>与zookeeper建立连接时客户端等待的最长时间。</td></tr><tr><td>zookeeper.sync.time.ms</td><td>2000</td><td>ZK follower可以罗ZK leader多久</td></tr><tr><td>offsets.storage</td><td>zookeeper</td><td>选择存储偏移量的位置（zookeeper或kafka）。</td></tr><tr><td>offsets.channel.backoff.ms</td><td>1000</td><td>重新连接offset通道或重试失败的偏移提取&#x2F;提交请求时的回退周期。</td></tr><tr><td>offsets.channel.socket.timeout.ms</td><td>10000</td><td>读取offset拉取&#x2F;提交响应的Socker的超时时间。此超时也用于查询offset manager的ConsumerMetadata请求。</td></tr><tr><td>offsets.commit.max.retries</td><td>5</td><td>失败时重试偏移提交的最大次数。此重试计数仅适用于停机期间的offset提交，它不适用于自动提交线程的提交。它也不适用于在提交offset之前查询偏移协调器的尝试。即如果消费者元数据请求由于任何原因而失败，则将重试它，并且重试不计入该限制。</td></tr><tr><td>dual.commit.enabled</td><td>true</td><td>如果使用“kafka”作为offsets.storage，则可以向ZooKeeper（除Kafka之外）进行双重提交offset。在从基于zookeeper的offset存储迁移到kafka存储的时候可以这么做。对于任何给定的消费者组，在该组中的所有实例已迁移到提交到broker（而不是直接到ZooKeeper）的新的版本之后，可以关闭这个。</td></tr><tr><td>partition.assignment.strategy</td><td>range</td><td>在“range”或“roundrobin”策略之间选择将分区分配给消费者流。 循环分区分配器分配所有可用的分区和所有可用的消费者线程。然后，继续从分区到消费者线程进行循环任务。如果所有消费者实例的订阅是相同的，则分区将被均匀分布。（即，分区所有权计数将在所有消费者线程之间的差异仅在一个delta之内。）循环分配仅在以下情况下被允许：（a）每个主题在消费者实例中具有相同数量的流（b）订阅的topic的对于组内的每个消费者实例都是相同的。 范围(Range)分区基于每个topic。对于每个主题，我们按数字顺序排列可用的分区，并以字典顺序排列消费者线程。然后，我们将分区数除以消费者流（线程）的总数来确定分配给每个消费者的分区数。如果不均匀分割，那么前几个消费者将会有多的分区。</td></tr></tbody></table><p>有关消费者配置的更多详细信息，请参见scaf类kafka.consumer.ConsumerConfig。</p><h1 id="Kafka-Producer配置"><a href="#Kafka-Producer配置" class="headerlink" title="Kafka Producer配置"></a>Kafka Producer配置</h1><h2 id="生产者配置"><a href="#生产者配置" class="headerlink" title="生产者配置"></a>生产者配置</h2><p>java生产者配置：</p><table><thead><tr><th>NAME</th><th>DESCRIPTION</th><th>TYPE</th><th>DEFAULT</th><th>VALID VALUES</th><th>IMPORTANCE</th></tr></thead><tbody><tr><td>bootstrap.servers</td><td>host&#x2F;port列表，用于初始化建立和Kafka集群的连接。列表格式为host1:port1,host2:port2,....，无需添加所有的集群地址，kafka会根据提供的地址发现其他的地址（你可以多提供几个，以防提供的服务器关闭）</td><td>list</td><td></td><td></td><td>high</td></tr><tr><td>key.serializer</td><td>实现 org.apache.kafka.common.serialization.Serializer 接口的 key 的 Serializer 类。</td><td>class</td><td></td><td></td><td>high</td></tr><tr><td>value.serializer</td><td>实现 org.apache.kafka.common.serialization.Serializer 接口的value 的 Serializer 类。</td><td>class</td><td></td><td></td><td>high</td></tr><tr><td>acks</td><td>生产者需要leader确认请求完成之前接收的应答数。此配置控制了发送消息的耐用性，支持以下配置： acks&#x3D;0 如果设置为0，那么生产者将不等待任何消息确认。消息将立刻添加到socket缓冲区并考虑发送。在这种情况下不能保障消息被服务器接收到。并且重试机制不会生效（因为客户端不知道故障了没有）。每个消息返回的offset始终设置为-1。 acks&#x3D;1，这意味着leader写入消息到本地日志就立即响应，而不等待所有follower应答。在这种情况下，如果响应消息之后但follower还未复制之前leader立即故障，那么消息将会丢失。 acks&#x3D;all 这意味着leader将等待所有副本同步后应答消息。此配置保障消息不会丢失（只要至少有一个同步的副本或者）。这是最强壮的可用性保障。等价于acks&#x3D;-1。</td><td>string</td><td>1</td><td>[all, -1, 0, 1]</td><td>high</td></tr><tr><td>buffer.memory</td><td>生产者用来缓存等待发送到服务器的消息的内存总字节数。如果消息发送比可传递到服务器的快，生产者将阻塞<code>max.block.ms</code>之后，抛出异常。 此设置应该大致的对应生产者将要使用的总内存，但不是硬约束，因为生产者所使用的所有内存都用于缓冲。一些额外的内存将用于压缩（如果启动压缩），以及用于保持发送中的请求。</td><td>long</td><td>33554432</td><td>[0,...]</td><td>high</td></tr><tr><td>compression.type</td><td>数据压缩的类型。默认为空（就是不压缩）。有效的值有 none，gzip，snappy, 或 lz4。压缩全部的数据批，因此批的效果也将影响压缩的比率（更多的批次意味着更好的压缩）。</td><td>string</td><td>none</td><td></td><td>high</td></tr><tr><td>retries</td><td>设置一个比零大的值，客户端如果发送失败则会重新发送。注意，这个重试功能和客户端在接到错误之后重新发送没什么不同。如果max.in.flight.requests.per.connection没有设置为1，有可能改变消息发送的顺序，因为如果2个批次发送到一个分区中，并第一个失败了并重试，但是第二个成功了，那么第二个批次将超过第一个。</td><td>int</td><td>0</td><td>[0,...,2147483647]</td><td>high</td></tr><tr><td>ssl.key.password</td><td>密钥仓库文件中的私钥的密码。</td><td>password</td><td>null</td><td></td><td>high</td></tr><tr><td>ssl.keystore.location</td><td>密钥仓库文件的位置。可用于客户端的双向认证。</td><td>string</td><td>null</td><td></td><td>high</td></tr><tr><td>ssl.keystore.password</td><td>密钥仓库文件的仓库密码。只有配置了ssl.keystore.location时才需要。</td><td>password</td><td>null</td><td></td><td>high</td></tr><tr><td>ssl.truststore.location</td><td>信任仓库的位置</td><td>string</td><td>null</td><td></td><td>high</td></tr><tr><td>ssl.truststore.password</td><td>信任仓库文件的密码</td><td>password</td><td>null</td><td></td><td>high</td></tr><tr><td>batch.size</td><td>当多个消息要发送到相同分区的时，生产者尝试将消息批量打包在一起，以减少请求交互。这样有助于客户端和服务端的性能提升。该配置的默认批次大小（以字节为单位）： 不会打包大于此配置大小的消息。 发送到broker的请求将包含多个批次，每个分区一个，用于发送数据。 较小的批次大小有可能降低吞吐量（批次大小为0则完全禁用批处理）。一个非常大的批次大小可能更浪费内存。因为我们会预先分配这个资源。</td><td>int</td><td>16384</td><td>[0,...]</td><td>medium</td></tr><tr><td>client.id</td><td>当发出请求时传递给服务器的id字符串。这样做的目的是允许服务器请求记录记录这个【逻辑应用名】，这样能够追踪请求的源，而不仅仅只是ip&#x2F;prot。</td><td>string</td><td>&quot;&quot;</td><td></td><td>medium</td></tr><tr><td>connections.max.idle.ms</td><td>多少毫秒之后关闭闲置的连接。</td><td>long</td><td>540000</td><td></td><td>medium</td></tr><tr><td>linger.ms</td><td>生产者组将发送的消息组合成单个批量请求。正常情况下，只有消息到达的速度比发送速度快的情况下才会出现。但是，在某些情况下，即使在适度的负载下，客户端也可能希望减少请求数量。此设置通过添加少量人为延迟来实现。- 也就是说，不是立即发出一个消息，生产者将等待一个给定的延迟，以便和其他的消息可以组合成一个批次。这类似于Nagle在TCP中的算法。此设置给出批量延迟的上限：一旦我们达到分区的batch.size值的记录，将立即发送，不管这个设置如何，但是，如果比这个小，我们将在指定的“linger”时间内等待更多的消息加入。此设置默认为0（即无延迟）。假设，设置 linger.ms&#x3D;5，将达到减少发送的请求数量的效果，但对于在没有负载情况，将增加5ms的延迟。</td><td>long</td><td>0</td><td>[0,...]</td><td>medium</td></tr><tr><td>max.block.ms</td><td>该配置控制 KafkaProducer.send() 和 KafkaProducer.partitionsFor() 将阻塞多长时间。此外这些方法被阻止，也可能是因为缓冲区已满或元数据不可用。在用户提供的序列化程序或分区器中的锁定不会计入此超时。</td><td>long</td><td>60000</td><td>[0,...]</td><td>medium</td></tr><tr><td>max.request.size</td><td>请求的最大大小（以字节为单位）。此设置将限制生产者的单个请求中发送的消息批次数，以避免发送过大的请求。这也是最大消息批量大小的上限。请注意，服务器拥有自己的批量大小，可能与此不同。</td><td>int</td><td>1048576</td><td>[0,...]</td><td>medium</td></tr><tr><td>partitioner.class</td><td>实现Partitioner接口的的Partitioner类。</td><td>class</td><td>org.apache.kafka.clients.producer.internals.DefaultPartitioner</td><td></td><td>medium</td></tr><tr><td>receive.buffer.bytes</td><td>读取数据时使用的TCP接收缓冲区(SO_RCVBUF)的大小。如果值为-1，则将使用OS默认值。</td><td>int</td><td>32768</td><td>[-1,...]</td><td>medium</td></tr><tr><td>request.timeout.ms</td><td>该配置控制客户端等待请求响应的最长时间。如果在超时之前未收到响应，客户端将在必要时重新发送请求，如果重试耗尽，则该请求将失败。 这应该大于replica.lag.time.max.ms，以减少由于不必要的生产者重试引起的消息重复的可能性。</td><td>int</td><td>30000</td><td>[0,...]</td><td>medium</td></tr><tr><td>sasl.jaas.config</td><td>JAAS配置文件使用的格式的SASL连接的JAAS登录上下文参数。<a href="/fwd?link=https://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/tutorials/LoginConfigFile.html">这里</a>描述JAAS配置文件格式。该值的格式为：&#39;（&#x3D;）*;&#39;</td><td>password</td><td>null</td><td></td><td>medium</td></tr><tr><td>sasl.kerberos.service.name</td><td>Kafka运行的Kerberos主体名称。可以在Kafka的JAAS配置或Kafka的配置中定义。</td><td>string</td><td>null</td><td></td><td>medium</td></tr><tr><td>sasl.mechanism</td><td>SASL机制用于客户端连接。这是安全提供者可用与任何机制。GSSAPI是默认机制。</td><td>string</td><td>GSSAPI</td><td></td><td>medium</td></tr><tr><td>security.protocol</td><td>用于与broker通讯的协议。 有效值为：PLAINTEXT，SSL，SASL_PLAINTEXT，SASL_SSL。</td><td>string</td><td>PLAINTEXT</td><td></td><td>medium</td></tr><tr><td>send.buffer.bytes</td><td>发送数据时，用于TCP发送缓存（SO_SNDBUF）的大小。如果值为 -1，将默认使用系统的。</td><td>int</td><td>131072</td><td>[-1,...]</td><td>medium</td></tr><tr><td>ssl.enabled.protocols</td><td>启用SSL连接的协议列表。</td><td>list</td><td>TLSv1.2,TLSv1.1,TLSv1</td><td></td><td>medium</td></tr><tr><td>ssl.keystore.type</td><td>密钥存储文件的文件格式。对于客户端是可选的。</td><td>string</td><td>JKS</td><td></td><td>medium</td></tr><tr><td>ssl.protocol</td><td>最近的JVM中允许的值是TLS，TLSv1.1和TLSv1.2。 较旧的JVM可能支持SSL，SSLv2和SSLv3，但由于已知的安全漏洞，不建议使用SSL。</td><td>string</td><td>TLS</td><td></td><td>medium</td></tr><tr><td>ssl.provider</td><td>用于SSL连接的安全提供程序的名称。默认值是JVM的默认安全提供程序。</td><td>string</td><td>null</td><td></td><td>medium</td></tr><tr><td>ssl.truststore.type</td><td>信任仓库文件的文件格式。</td><td>string</td><td>JKS</td><td></td><td>medium</td></tr><tr><td>enable.idempotence</td><td>当设置为‘true’，生产者将确保每个消息正好一次复制写入到stream。如果‘false’，由于broker故障，生产者重试。即，可以在流中写入重试的消息。此设置默认是‘false’。请注意，启用幂等式需要将max.in.flight.requests.per.connection设置为1，重试次数不能为零。另外acks必须设置为“全部”。如果这些值保持默认值，我们将覆盖默认值。 如果这些值设置为与幂等生成器不兼容的值，则将抛出一个ConfigException异常。如果这些值设置为与幂等生成器不兼容的值，则将抛出一个ConfigException异常。</td><td>boolean</td><td>false</td><td></td><td>low</td></tr><tr><td>interceptor.classes</td><td>实现ProducerInterceptor接口，你可以在生产者发布到Kafka群集之前拦截（也可变更）生产者收到的消息。默认情况下没有拦截器。</td><td>list</td><td>null</td><td></td><td>low</td></tr><tr><td>max.in.flight.requests.per.connection</td><td>阻塞之前，客户端单个连接上发送的未应答请求的最大数量。注意，如果此设置设置大于1且发送失败，则会由于重试（如果启用了重试）会导致消息重新排序的风险。</td><td>int</td><td>5</td><td>[1,...]</td><td>low</td></tr><tr><td>metadata.max.age.ms</td><td>在一段时间段之后（以毫秒为单位），强制更新元数据，即使我们没有看到任何分区leader的变化，也会主动去发现新的broker或分区。</td><td>long</td><td>300000</td><td>[0,...]</td><td>low</td></tr><tr><td>metric.reporters</td><td>用作metrics reporters（指标记录员）的类的列表。实现MetricReporter接口，将受到新增加的度量标准创建类插入的通知。 JmxReporter始终包含在注册JMX统计信息中。</td><td>list</td><td>&quot;&quot;</td><td></td><td>low</td></tr><tr><td>metrics.num.samples</td><td>维护用于计算度量的样例数量。</td><td>int</td><td>2</td><td>[1,...]</td><td>low</td></tr><tr><td>metrics.recording.level</td><td>指标的最高记录级别。</td><td>string</td><td>INFO</td><td>[INFO, DEBUG]</td><td>low</td></tr><tr><td>metrics.sample.window.ms</td><td>度量样例计算上</td><td>long</td><td>30000</td><td>[0,...]</td><td>low</td></tr><tr><td>reconnect.backoff.max.ms</td><td>重新连接到重复无法连接的代理程序时等待的最大时间（毫秒）。 如果提供，每个主机的回退将会连续增加，直到达到最大值。 计算后退增加后，增加20％的随机抖动以避免连接风暴。</td><td>long</td><td>1000</td><td>[0,...]</td><td>low</td></tr><tr><td>reconnect.backoff.ms</td><td>尝试重新连接到给定主机之前等待的基本时间量。这避免了在循环中高频率的重复连接到主机。这种回退适应于客户端对broker的所有连接尝试。</td><td>long</td><td>50</td><td>[0,...]</td><td>low</td></tr><tr><td>retry.backoff.ms</td><td>尝试重试指定topic分区的失败请求之前等待的时间。这样可以避免在某些故障情况下高频次的重复发送请求。</td><td>long</td><td>100</td><td>[0,...]</td><td>low</td></tr><tr><td>sasl.kerberos.kinit.cmd</td><td>Kerberos kinit 命令路径。</td><td>string</td><td>&#x2F;usr&#x2F;bin&#x2F;kinit</td><td></td><td>low</td></tr><tr><td>sasl.kerberos.min.time.before.relogin</td><td>Login线程刷新尝试之间的休眠时间。</td><td>long</td><td>60000</td><td></td><td>low</td></tr><tr><td>sasl.kerberos.ticket.renew.jitter</td><td>添加更新时间的随机抖动百分比。</td><td>double</td><td>0.05</td><td></td><td>low</td></tr><tr><td>sasl.kerberos.ticket.renew.window.factor</td><td>登录线程将睡眠，直到从上次刷新ticket到期时间的指定窗口因子为止，此时将尝试续订ticket。</td><td>double</td><td>0.8</td><td></td><td>low</td></tr><tr><td>ssl.cipher.suites</td><td>密码套件列表。这是使用TLS或SSL网络协议来协商用于网络连接的安全设置的认证，加密，MAC和密钥交换算法的命名组合。默认情况下，支持所有可用的密码套件。</td><td>list</td><td>null</td><td></td><td>low</td></tr><tr><td>ssl.endpoint.identification.algorithm</td><td>使用服务器证书验证服务器主机名的端点识别算法。</td><td>string</td><td>null</td><td></td><td>low</td></tr><tr><td>ssl.keymanager.algorithm</td><td>用于SSL连接的密钥管理因子算法。默认值是为Java虚拟机配置的密钥管理器工厂算法。</td><td>string</td><td>SunX509</td><td></td><td>low</td></tr><tr><td>ssl.secure.random.implementation</td><td>用于SSL加密操作的SecureRandom PRNG实现。</td><td>string</td><td>null</td><td></td><td>low</td></tr><tr><td>ssl.trustmanager.algorithm</td><td>用于SSL连接的信任管理因子算法。默认值是JAVA虚拟机配置的信任管理工厂算法。</td><td>string</td><td>PKIX</td><td></td><td>low</td></tr><tr><td>transaction.timeout.ms</td><td>生产者在主动中止正在进行的交易之前，交易协调器等待事务状态更新的最大时间（以ms为单位）。如果此值大于broker中的max.transaction.timeout.ms设置，则请求将失败，并报“InvalidTransactionTimeout”错误。</td><td>int</td><td>60000</td><td></td><td>low</td></tr><tr><td>transactional.id</td><td>用于事务传递的TransactionalId。这样可以跨多个生产者会话的可靠性语义，因为它允许客户端保证在开始任何新事务之前使用相同的TransactionalId的事务已经完成。如果没有提供TransactionalId，则生产者被限制为幂等传递。请注意，如果配置了TransactionalId，则必须启用enable.idempotence。 默认值为空，这意味着无法使用事务。</td><td>string</td><td>null</td><td>non-empty string</td><td>low</td></tr></tbody></table><h2 id="kafka-2-0-0-1"><a href="#kafka-2-0-0-1" class="headerlink" title="kafka &gt;&#x3D; 2.0.0"></a>kafka &gt;&#x3D; 2.0.0</h2><table><thead><tr><th>名称</th><th>描述</th><th>类型</th><th>默认</th><th>有效值</th><th>重要程度</th></tr></thead><tbody><tr><td>sasl.client.callback.handler.class</td><td>实现AuthenticateCallbackHandler接口的SASL客户端回调处理程序类的全称。</td><td>class</td><td>null</td><td></td><td>中间</td></tr><tr><td>sasl.login.callback.handler.class</td><td>实现AuthenticateCallbackHandler接口的SASL登录回调处理程序类的全称。对于broker来说，登录回调处理程序配置必须以监听器前缀和小写的SASL机制名称为前缀。例如，<code>listener.name.sasl_ssl.scram-sha-256.sasl.login.callback.handler.class=com.example.CustomScramLoginCallbackHandler</code></td><td>class</td><td>null</td><td></td><td>中间</td></tr><tr><td>sasl.login.class</td><td>实现Login接口的类的全称。对于broker来说，login config必须以监听器前缀和SASL机制名称为前缀，并使用小写。例如，<code>listener.name.sasl_ssl.scram-sha-256.sasl.login.class=com.example.CustomScramLogin</code>。</td><td>class</td><td>null</td><td></td><td>中间</td></tr></tbody></table><h2 id="kafka-2-1-0-1"><a href="#kafka-2-1-0-1" class="headerlink" title="kafka &gt;&#x3D; 2.1.0"></a>kafka &gt;&#x3D; 2.1.0</h2><table><thead><tr><th>名称</th><th>描述</th><th>类型</th><th>默认</th><th>有效值</th><th>重要程度</th></tr></thead><tbody><tr><td>client.dns.lookup</td><td>控制客户端如何使用DNS查询。如果设置为 <code>use_all_dns_ips</code>，则依次连接到每个返回的IP地址，直到成功建立连接。断开连接后，使用下一个IP。一旦所有的IP都被使用过一次，客户端就会再次从主机名中解析IP(s)(然而，JVM和操作系统都会缓存DNS名称查询)。如果设置为 <code>resolve_canonical_bootstrap_servers_only</code>，则将每个引导地址解析成一个canonical名称列表。在bootstrap阶段之后，这和<code>use_all_dns_ips</code>的行为是一样的。如果设置为 <code>default</code>(已弃用)，则尝试连接到查找返回的第一个IP地址，即使查找返回多个IP地址。</td><td>string</td><td>use_all_dns_ips</td><td><code>[default, use_all_dns_ips, resolve_canonical_bootstrap_servers_only]</code></td><td>中间</td></tr><tr><td>delivery.timeout.ms</td><td>调用<code>send()</code>返回后报告成功或失败的时间上限。这限制了消息在发送前被延迟的总时间，等待broker确认的时间（如果期望的话），以及允许重试发送失败的时间。如果遇到不可恢复的错误，重试次数已经用尽，或者消息被添加到一个达到较早发送到期期限的批次中，生产者可能会报告未能在这个配置之前发送记录。这个配置的值应该大于或等于<code>request.timeout.ms</code>和<code>linger.ms</code>之和。</td><td>int</td><td>120000 (2 minutes)</td><td><code>[0,...]</code></td><td>中间</td></tr></tbody></table><h2 id="kafka-2-7-1"><a href="#kafka-2-7-1" class="headerlink" title="kafka &gt;&#x3D; 2.7"></a>kafka &gt;&#x3D; 2.7</h2><table><thead><tr><th>名称</th><th>描述</th><th>类型</th><th>默认</th><th>有效值</th><th>重要程度</th></tr></thead><tbody><tr><td>ssl.truststore.certificates</td><td>可信证书的格式由&#39;ssl.truststore.type&#39;指定。默认的SSL引擎工厂只支持带X.509证书的PEM格式。</td><td>password</td><td>null</td><td></td><td>高</td></tr><tr><td>socket.connection.setup.timeout.max.ms</td><td>客户端等待建立socket连接的最大时间。连接设置超时时间将随着每一次连续的连接失败而成倍增加，直到这个最大值。为了避免连接风暴，超时时间将被应用一个0.2的随机因子，导致计算值在20%以下和20%以上的随机范围。</td><td>long</td><td>127000 (127 seconds)</td><td></td><td>中间</td></tr><tr><td>socket.connection.setup.timeout.ms</td><td>客户端等待建立socket连接的时间。如果在超时之前没有建立连接，客户端将关闭socket通道。</td><td>long</td><td>10000 (10 seconds)</td><td></td><td>中间</td></tr></tbody></table></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者:</span> <span class="post-copyright-info"><a href="https://xiaowu95.wang">小五</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接:</span> <span class="post-copyright-info"><a href="https://xiaowu95.wang/posts/f351760e/">https://xiaowu95.wang/posts/f351760e/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明:</span> <span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://xiaowu95.wang" target="_blank">小五的个人杂货铺</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/kafka/">kafka</a><a class="post-meta__tags" href="/tags/mq/">mq</a></div><div class="post-share"><div class="social-share" data-image="/img/nba-logo19.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="post-reward"><div class="reward-button"><i class="fas fa-qrcode"></i>感谢支持</div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wxpay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/loading.gif" data-lazy-src="/img/wxpay.jpg" alt="微信"></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/loading.gif" data-lazy-src="/img/alipay.jpg" alt="支付宝"></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/posts/221dce9c/" title="kafka生产调优"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/nba-logo4.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of previous post"><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">kafka生产调优</div></div><div class="info-2"><div class="info-item-1">一、Kafka 硬件 配置选择1、场景说明 2、服务器台数选择 3、磁盘选择 4、内存选择Kafka 内存组成：堆内存 + 页缓存 1）Kafka 堆内存建议每个节点：10g ~ 15g 在 kafka-server-start.sh 中修改 123if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then export KAFKA_HEAP_OPTS=&quot;-Xmx10G -Xms10G&quot;fi 查看 Kafka 进程号: 1234[atguigu@hadoop102 kafka]$ jps2321 Kafka5255 Jps1931 QuorumPeerMain 根据 Kafka 进程号，查看 Kafka 的 GC 情况: 1jstat -gc 2321 1s 10 新生代GC次数 根据 Kafka 进程号，查看 Kafka...</div></div></div></a><a class="pagination-related" href="/posts/a3d4c6bc/" title="Kafka集群搭建(含ZK模式和Kraft模式)"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/nba-logo2.jpg" onerror='onerror=null,src="/img/404.jpg"' alt="cover of next post"><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Kafka集群搭建(含ZK模式和Kraft模式)</div></div><div class="info-2"><div class="info-item-1">前言环境介绍虚拟机软件：VirtualBox Linux 发行版本：Ubuntu 20.04.4 虚拟机核心数：1 core 虚拟机内存：2 GB JDK 版本：1.8.0_202 ZK 版本：3.8.0 Kafka 版本：3.2.0 Kafka - ZK 模式Kafka 2.8.0 之前，所有元数据信息都存储在 ZK。 ZK 成为 Kafka 瓶颈。从 2.8.0 开始，可以将元数据信息存储在 Kafka，脱离 ZK。 集群规划 node01 node02 node03 zk zk zk kafka kafka kafka ZK 集群部署可以参考 《Hadoop HA 搭建》 中的 ZK 集群搭建 Kafka 集群部署Kafka 环境变量1234567$ vim /etc/profile# 尾部添加以下内容export KAFKA_HOME=/opt/kafka-3.2.0export PATH=$PATH:$KAFKA_HOME/bin$ xsync $KAFKA_HOME$ xsync /etc/profile 配置...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/posts/96b6e5e9/" title="Kafka-Eagle监控&amp;Kraft模式"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/nba-logo22.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2023-06-15</div><div class="info-item-2">Kafka-Eagle监控&amp;Kraft模式</div></div><div class="info-2"><div class="info-item-1">Kafka-Eagle监控Kafka-Eagle框架可以监控 Kafka 集群的整体运行情况，在生产环境中经常使用。 在此之前监控工具需要MySQL作为持久化手段。 一、Kafka环境准备1、关闭 Kafka 集群12# 停止集群kf.sh stop 2、修改&#x2F;opt&#x2F;module&#x2F;kafka&#x2F;bin&#x2F;kafka-server-start.sh1vim bin/kafka-server-start.sh 修改如下参数值： 123if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then export KAFKA_HEAP_OPTS=&quot;-Xmx1G -Xms1G&quot;fi 为 12345if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then export KAFKA_HEAP_OPTS=&quot;-server -Xms2G -Xmx2G -XX:PermSize=128m ...</div></div></div></a><a class="pagination-related" href="/posts/a3d4c6bc/" title="Kafka集群搭建(含ZK模式和Kraft模式)"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/nba-logo2.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2023-06-15</div><div class="info-item-2">Kafka集群搭建(含ZK模式和Kraft模式)</div></div><div class="info-2"><div class="info-item-1">前言环境介绍虚拟机软件：VirtualBox Linux 发行版本：Ubuntu 20.04.4 虚拟机核心数：1 core 虚拟机内存：2 GB JDK 版本：1.8.0_202 ZK 版本：3.8.0 Kafka 版本：3.2.0 Kafka - ZK 模式Kafka 2.8.0 之前，所有元数据信息都存储在 ZK。 ZK 成为 Kafka 瓶颈。从 2.8.0 开始，可以将元数据信息存储在 Kafka，脱离 ZK。 集群规划 node01 node02 node03 zk zk zk kafka kafka kafka ZK 集群部署可以参考 《Hadoop HA 搭建》 中的 ZK 集群搭建 Kafka 集群部署Kafka 环境变量1234567$ vim /etc/profile# 尾部添加以下内容export KAFKA_HOME=/opt/kafka-3.2.0export PATH=$PATH:$KAFKA_HOME/bin$ xsync $KAFKA_HOME$ xsync /etc/profile 配置...</div></div></div></a><a class="pagination-related" href="/posts/221dce9c/" title="kafka生产调优"><img class="cover" src="/img/loading.gif" data-lazy-src="/img/nba-logo4.jpg" alt="cover"><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="fas fa-history fa-fw"></i> 2023-06-15</div><div class="info-item-2">kafka生产调优</div></div><div class="info-2"><div class="info-item-1">一、Kafka 硬件 配置选择1、场景说明 2、服务器台数选择 3、磁盘选择 4、内存选择Kafka 内存组成：堆内存 + 页缓存 1）Kafka 堆内存建议每个节点：10g ~ 15g 在 kafka-server-start.sh 中修改 123if [ &quot;x$KAFKA_HEAP_OPTS&quot; = &quot;x&quot; ]; then export KAFKA_HEAP_OPTS=&quot;-Xmx10G -Xms10G&quot;fi 查看 Kafka 进程号: 1234[atguigu@hadoop102 kafka]$ jps2321 Kafka5255 Jps1931 QuorumPeerMain 根据 Kafka 进程号，查看 Kafka 的 GC 情况: 1jstat -gc 2321 1s 10 新生代GC次数 根据 Kafka 进程号，查看 Kafka...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/loading.gif" data-lazy-src="/img/avatar.png" onerror='this.onerror=null,this.src="/img/friend_404.gif"' alt="avatar"></div><div class="author-info-name">小五</div><div class="author-info-description">Tomorrow will be better,Everything will be fine</div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">550</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">163</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">58</div></a></div><a id="card-info-btn" target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/wang-xiaowu"><i class="fab fa-github"></i><span>GitHub</span></a><div class="card-info-social-icons"><a class="social-icon" href="/random" target="_blank" title="随便逛逛"><i class="fa-solid fa-shuffle"></i></a><a class="social-icon" href="mailto:wangxiaowu950330@foxmail.com" rel="external nofollow noreferrer" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://space.bilibili.com/30655189?spm_id_from=333.1007.0.0" rel="external nofollow noreferrer" target="_blank" title="BiliBili"><i class="fa-brands fa-bilibili"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fa-solid fa-rss"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka%E6%A6%82%E8%BF%B0"><span class="toc-number">1.</span> <span class="toc-text">Kafka概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">1.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97"><span class="toc-number">1.2.</span> <span class="toc-text">消息队列</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-number">1.2.1.</span> <span class="toc-text">传统消息队列的应用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.2.2.</span> <span class="toc-text">消息队列的两种模式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="toc-number">1.3.</span> <span class="toc-text">基础架构</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8"><span class="toc-number">2.</span> <span class="toc-text">Kafka 快速入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-number">2.1.</span> <span class="toc-text">安装部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92"><span class="toc-number">2.1.1.</span> <span class="toc-text">集群规划</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%AF%E5%8A%A8"><span class="toc-number">2.1.2.</span> <span class="toc-text">启动</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.</span> <span class="toc-text">Kafka命令行操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E9%A2%98%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.1.</span> <span class="toc-text">主题命令行操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.2.</span> <span class="toc-text">生产者命令行操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.3.</span> <span class="toc-text">消费者命令行操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-%E7%94%9F%E4%BA%A7%E8%80%85"><span class="toc-number">3.</span> <span class="toc-text">Kafka 生产者</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B"><span class="toc-number">3.1.</span> <span class="toc-text">生产者消息发送流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%91%E9%80%81%E5%8E%9F%E7%90%86"><span class="toc-number">3.1.1.</span> <span class="toc-text">发送原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0%E5%88%97%E8%A1%A8"><span class="toc-number">3.1.2.</span> <span class="toc-text">生产者重要参数列表</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%91%E9%80%81"><span class="toc-number">3.2.</span> <span class="toc-text">发送</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%99%AE%E9%80%9A%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81"><span class="toc-number">3.2.1.</span> <span class="toc-text">普通异步发送</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%A6%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0%E7%9A%84%E5%BC%82%E6%AD%A5%E5%8F%91%E9%80%81"><span class="toc-number">3.2.2.</span> <span class="toc-text">带回调函数的异步发送</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%90%8C%E6%AD%A5%E5%8F%91%E9%80%81API"><span class="toc-number">3.2.3.</span> <span class="toc-text">同步发送API</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E6%8B%A6%E6%88%AA%E5%99%A8"><span class="toc-number">3.3.</span> <span class="toc-text">生产者拦截器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E5%88%86%E5%8C%BA"><span class="toc-number">3.4.</span> <span class="toc-text">生产者分区</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E7%9A%84%E5%A5%BD%E5%A4%84"><span class="toc-number">3.4.1.</span> <span class="toc-text">分区的好处</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E5%88%86%E5%8C%BA%E7%AD%96%E7%95%A5"><span class="toc-number">3.4.2.</span> <span class="toc-text">生产者发送消息分区策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA%E5%99%A8"><span class="toc-number">3.4.3.</span> <span class="toc-text">自定义分区器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E6%8F%90%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F"><span class="toc-number">3.5.</span> <span class="toc-text">生产者提高吞吐量</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="toc-number">3.5.1.</span> <span class="toc-text">消息累加器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E7%BA%BF%E7%A8%8B%EF%BC%88Sender%EF%BC%89"><span class="toc-number">3.5.2.</span> <span class="toc-text">消息发送线程（Sender）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E7%BB%8F%E9%AA%8C%E2%80%94%E6%95%B0%E6%8D%AE%E5%8F%AF%E9%9D%A0%E6%80%A7"><span class="toc-number">3.6.</span> <span class="toc-text">生产经验—数据可靠性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E7%A1%AE%E8%AE%A4%E6%9C%BA%E5%88%B6-ACK"><span class="toc-number">3.6.1.</span> <span class="toc-text">消息确认机制-ACK</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%8E%BB%E9%87%8D-%E5%B9%82%E7%AD%89%E6%80%A7"><span class="toc-number">3.6.2.</span> <span class="toc-text">数据去重-幂等性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E4%BA%8B%E5%8A%A1"><span class="toc-number">3.6.3.</span> <span class="toc-text">消息事务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E6%81%AF%E9%A1%BA%E5%BA%8F"><span class="toc-number">3.7.</span> <span class="toc-text">消息顺序</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-Broker"><span class="toc-number">4.</span> <span class="toc-text">Kafka Broker</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Broker%E8%AE%BE%E8%AE%A1"><span class="toc-number">4.1.</span> <span class="toc-text">Broker设计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Zookeeper"><span class="toc-number">4.2.</span> <span class="toc-text">Zookeeper</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Zookeeper%E4%BD%9C%E7%94%A8"><span class="toc-number">4.2.1.</span> <span class="toc-text">Zookeeper作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Broker%E9%80%89%E4%B8%BELeader"><span class="toc-number">4.2.2.</span> <span class="toc-text">Broker选举Leader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Broker%E9%87%8D%E8%A6%81%E5%8F%82%E6%95%B0"><span class="toc-number">4.2.3.</span> <span class="toc-text">Broker重要参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%8A%82%E7%82%B9%E6%9C%8D%E5%BD%B9%E5%92%8C%E9%80%80%E5%BD%B9"><span class="toc-number">4.3.</span> <span class="toc-text">节点服役和退役</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%8D%E5%BD%B9%E6%96%B0%E8%8A%82%E7%82%B9"><span class="toc-number">4.3.1.</span> <span class="toc-text">服役新节点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%80%E5%BD%B9%E6%97%A7%E8%8A%82%E7%82%B9"><span class="toc-number">4.3.2.</span> <span class="toc-text">退役旧节点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6"><span class="toc-number">4.4.</span> <span class="toc-text">副本机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-number">4.4.1.</span> <span class="toc-text">副本基本信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E9%80%89%E4%B8%BELeader"><span class="toc-number">4.4.2.</span> <span class="toc-text">副本选举Leader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%89%AF%E6%9C%AC%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86"><span class="toc-number">4.4.3.</span> <span class="toc-text">副本故障处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#follower%E6%95%85%E9%9A%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">4.4.3.1.</span> <span class="toc-text">follower故障流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#leader%E6%95%85%E9%9A%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">4.4.3.2.</span> <span class="toc-text">leader故障流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%AC%E5%88%86%E9%85%8D"><span class="toc-number">4.4.4.</span> <span class="toc-text">分区副本分配</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E8%B0%83%E6%95%B4%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%AC"><span class="toc-number">4.4.5.</span> <span class="toc-text">手动调整分区副本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E8%87%AA%E5%8A%A8%E8%B0%83%E6%95%B4"><span class="toc-number">4.4.6.</span> <span class="toc-text">分区自动调整</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A2%9E%E5%8A%A0%E5%89%AF%E6%9C%AC%E5%9B%A0%E5%AD%90"><span class="toc-number">4.4.7.</span> <span class="toc-text">增加副本因子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8"><span class="toc-number">4.5.</span> <span class="toc-text">文件存储</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AD%98%E5%82%A8%E7%BB%93%E6%9E%84"><span class="toc-number">4.5.1.</span> <span class="toc-text">存储结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E4%BB%B6%E6%B8%85%E7%90%86%E7%AD%96%E7%95%A5"><span class="toc-number">4.5.2.</span> <span class="toc-text">****文件清理策略</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E5%88%A0%E9%99%A4"><span class="toc-number">4.5.2.1.</span> <span class="toc-text">日志删除</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9"><span class="toc-number">4.5.2.2.</span> <span class="toc-text">日志压缩</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Kafka%E9%AB%98%E6%95%88%E8%AF%BB%E6%95%B0%E6%8D%AE"><span class="toc-number">4.6.</span> <span class="toc-text">Kafka高效读数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%BA%E5%BA%8F%E5%86%99%E7%A3%81%E7%9B%98"><span class="toc-number">4.6.1.</span> <span class="toc-text">顺序写磁盘</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B5%E7%BC%93%E5%AD%98%E4%B8%8E%E9%9B%B6%E6%8B%B7%E8%B4%9D"><span class="toc-number">4.6.2.</span> <span class="toc-text">页缓存与零拷贝</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A1%B5%E7%BC%93%E5%AD%98"><span class="toc-number">4.6.2.1.</span> <span class="toc-text">页缓存</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%9B%B6%E6%8B%B7%E8%B4%9D"><span class="toc-number">4.6.2.2.</span> <span class="toc-text">零拷贝</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-number">5.</span> <span class="toc-text">Kafka消费者</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F"><span class="toc-number">5.1.</span> <span class="toc-text">消费模式</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">5.2.</span> <span class="toc-text">消费工作流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E6%80%BB%E4%BD%93%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">5.2.1.</span> <span class="toc-text">消费者总体工作流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E5%8E%9F%E7%90%86"><span class="toc-number">5.2.2.</span> <span class="toc-text">消费者组原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84%E9%80%89%E4%B8%BELeader"><span class="toc-number">5.2.3.</span> <span class="toc-text">消费者组选举Leader</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85API"><span class="toc-number">5.3.</span> <span class="toc-text">消费者API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E5%B9%B3%E8%A1%A1%E4%BB%A5%E5%8F%8A%E5%86%8D%E5%B9%B3%E8%A1%A1"><span class="toc-number">5.4.</span> <span class="toc-text">分区平衡以及再平衡</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D%E7%AD%96%E7%95%A5"><span class="toc-number">5.4.1.</span> <span class="toc-text">分区分配策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Range"><span class="toc-number">5.4.2.</span> <span class="toc-text">Range</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RoundRobin"><span class="toc-number">5.4.3.</span> <span class="toc-text">RoundRobin</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sticky%EF%BC%9A"><span class="toc-number">5.4.4.</span> <span class="toc-text">Sticky：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#offset%E4%BD%8D%E7%A7%BB%E6%8F%90%E4%BA%A4"><span class="toc-number">5.5.</span> <span class="toc-text">offset位移提交</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#offset-%E7%9A%84%E9%BB%98%E8%AE%A4%E7%BB%B4%E6%8A%A4%E4%BD%8D%E7%BD%AE"><span class="toc-number">5.5.1.</span> <span class="toc-text">offset 的默认维护位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%8F%90%E4%BA%A4"><span class="toc-number">5.5.2.</span> <span class="toc-text">自动提交</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E6%8F%90%E4%BA%A4"><span class="toc-number">5.5.3.</span> <span class="toc-text">手动提交</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E5%AE%9A%E6%B6%88%E8%B4%B9%E4%BD%8D%E7%BD%AE"><span class="toc-number">5.5.4.</span> <span class="toc-text">指定消费位置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BC%8F%E6%B6%88%E8%B4%B9%E5%92%8C%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9"><span class="toc-number">5.5.5.</span> <span class="toc-text">漏消费和重复消费</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B6%88%E8%B4%B9%E8%80%85%E4%BA%8B%E5%8A%A1"><span class="toc-number">5.6.</span> <span class="toc-text">消费者事务</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%A7%AF%E5%8E%8B%EF%BC%88%E6%8F%90%E9%AB%98%E5%90%9E%E5%90%90%E9%87%8F%EF%BC%89"><span class="toc-number">5.7.</span> <span class="toc-text">数据积压（提高吞吐量）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8B%A6%E6%88%AA%E5%99%A8"><span class="toc-number">5.8.</span> <span class="toc-text">拦截器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka%E6%95%B4%E5%90%88Spring-Boot"><span class="toc-number">6.</span> <span class="toc-text">Kafka整合Spring Boot</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-Consumer%E9%85%8D%E7%BD%AE"><span class="toc-number">7.</span> <span class="toc-text">Kafka Consumer配置</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B0%E6%B6%88%E8%B4%B9%E8%80%85%E9%85%8D%E7%BD%AE"><span class="toc-number">7.1.</span> <span class="toc-text">新消费者配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka-2-0-0"><span class="toc-number">7.2.</span> <span class="toc-text">kafka &gt;&#x3D; 2.0.0</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka-2-1-0"><span class="toc-number">7.3.</span> <span class="toc-text">kafka &gt;&#x3D; 2.1.0</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka-2-7"><span class="toc-number">7.4.</span> <span class="toc-text">kafka &gt;&#x3D; 2.7</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%97%A7%E6%B6%88%E8%B4%B9%E8%80%85%E9%85%8D%E7%BD%AE"><span class="toc-number">7.5.</span> <span class="toc-text">旧消费者配置</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Kafka-Producer%E9%85%8D%E7%BD%AE"><span class="toc-number">8.</span> <span class="toc-text">Kafka Producer配置</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E4%BA%A7%E8%80%85%E9%85%8D%E7%BD%AE"><span class="toc-number">8.1.</span> <span class="toc-text">生产者配置</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka-2-0-0-1"><span class="toc-number">8.2.</span> <span class="toc-text">kafka &gt;&#x3D; 2.0.0</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka-2-1-0-1"><span class="toc-number">8.3.</span> <span class="toc-text">kafka &gt;&#x3D; 2.1.0</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#kafka-2-7-1"><span class="toc-number">8.4.</span> <span class="toc-text">kafka &gt;&#x3D; 2.7</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/c75c82a0/" title="使用 TypeScript 创建 Koa 服务器"><img src="/img/loading.gif" data-lazy-src="/img/nba-logo16.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="使用 TypeScript 创建 Koa 服务器"></a><div class="content"><a class="title" href="/posts/c75c82a0/" title="使用 TypeScript 创建 Koa 服务器">使用 TypeScript 创建 Koa 服务器</a><time datetime="2025-10-11T06:00:11.000Z" title="发表于 2025-10-11 14:00:11">2025-10-11</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/c9d8bfa7/" title="Nodejs应用提取heap并加以分析的一些常用方法"><img src="/img/loading.gif" data-lazy-src="/img/nba-logo24.jpg" onerror='this.onerror=null,this.src="/img/404.jpg"' alt="Nodejs应用提取heap并加以分析的一些常用方法"></a><div class="content"><a class="title" href="/posts/c9d8bfa7/" title="Nodejs应用提取heap并加以分析的一些常用方法">Nodejs应用提取heap并加以分析的一些常用方法</a><time datetime="2025-09-28T07:54:43.000Z" title="发表于 2025-09-28 15:54:43">2025-09-28</time></div></div></div></div></div></div></main><footer id="footer" style="background-image:url(/img/nba-logo19.jpg)"><div id="footer-wrap"><div class="copyright">&copy;2019 - 2025 By 小五</div><div class="framework-info"><span>框架</span> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题</span> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a href="https://www.foreverblog.cn/" rel="external nofollow noreferrer" target="_blank"><img src="/img/loading.gif" data-lazy-src="https://img.foreverblog.cn/logo_en_default.png" alt="十年之约" style="width:auto;height:16px"></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@5.2.2/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@5.2.2/source/js/main.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@5.2.2/source/js/tw_cn.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.36/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload@19.1.3/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar@0.1.16/dist/snackbar.min.js"></script><div class="js-pjax"><script>(()=>{const e=()=>{const e=document.querySelectorAll("#article-container .mermaid-wrap");if(0===e.length)return;const t=()=>(e=>{window.loadMermaid=!0;const t="dark"===document.documentElement.getAttribute("data-theme")?"dark":"default";e.forEach(((e,n)=>{const d=e.firstElementChild,a=`mermaid-${n}`,r=`%%{init:{ 'theme':'${t}'}}%%\n`+d.textContent,i=mermaid.render(a,r),m=e=>{d.insertAdjacentHTML("afterend",e)};"string"==typeof i?m(i):i.then((({svg:e})=>m(e)))}))})(e);btf.addGlobalFn("themeChange",t,"mermaid"),window.loadMermaid?t():btf.getScript("https://cdn.jsdelivr.net/npm/mermaid@11.4.0/dist/mermaid.min.js").then(t)};btf.addGlobalFn("encrypt",e,"mermaid"),window.pjax?e():document.addEventListener("DOMContentLoaded",e)})()</script><script>(()=>{const t=document.querySelectorAll("#article-container .chartjs-container");if(0===t.length)return;const e=(t,a)=>{"object"==typeof t&&null!==t&&Object.keys(t).forEach((r=>{const n=t[r];"object"==typeof n&&null!==n&&(n[a]?t[r]=n[a]:e(n,a))}))},a=()=>{window.loadChartJS=!0,Array.from(t).forEach(((t,a)=>{const r=t.firstElementChild,n=t.getAttribute("data-chartjs-id")||"chartjs-"+a,d=t.getAttribute("data-width"),o=document.getElementById(n);o&&o.parentNode.remove();const c=r.textContent,l=document.createElement("canvas");l.id=n;const s=document.createElement("div");s.className="chartjs-wrap",d&&(s.style.width=d),s.appendChild(l),r.insertAdjacentElement("afterend",s);const h=document.getElementById(n).getContext("2d"),i=JSON.parse(c),m="dark"===document.documentElement.getAttribute("data-theme")?"dark-mode":"light-mode";(t=>{"dark-mode"===t?(Chart.defaults.color="rgba(255, 255, 255, 0.8)",Chart.defaults.borderColor="rgba(255, 255, 255, 0.2)",Chart.defaults.scale.ticks.backdropColor="transparent"):(Chart.defaults.color="rgba(0, 0, 0, 0.8)",Chart.defaults.borderColor="rgba(0, 0, 0, 0.1)",Chart.defaults.scale.ticks.backdropColor="transparent")})(m),e(i,m),new Chart(h,i)}))},r=()=>{window.loadChartJS?a():btf.getScript("https://cdn.jsdelivr.net/npm/chart.js@4.4.6/dist/chart.umd.min.js").then(a)};btf.addGlobalFn("themeChange",a,"chartjs"),btf.addGlobalFn("encrypt",a,"chartjs"),window.pjax?r():document.addEventListener("DOMContentLoaded",r)})()</script></div><div class="aplayer no-destroy" data-id="9061017364" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listfolded="false" data-order="random" data-lrctype="1" data-preload="none" data-autoplay="false" muted></div><div class="app-refresh" id="app-refresh"><div class="app-refresh-wrap"><label>网站已更新最新版本</label> <a href="javascript:void(0)" rel="external nofollow noreferrer" onclick="location.reload()">点击刷新</a></div></div><script>function showNotification(){if(GLOBAL_CONFIG.Snackbar){var t="light"===document.documentElement.getAttribute("data-theme")?GLOBAL_CONFIG.Snackbar.bgLight:GLOBAL_CONFIG.Snackbar.bgDark,e=GLOBAL_CONFIG.Snackbar.position;Snackbar.show({text:"已更新最新版本",backgroundColor:t,duration:5e5,pos:e,actionText:"点击刷新",actionTextColor:"#fff",onActionClick:function(t){location.reload()}})}else{var o=`top: 0; background: ${"light"===document.documentElement.getAttribute("data-theme")?"#49b1f5":"#1f1f1f"};`;document.getElementById("app-refresh").style.cssText=o}}"serviceWorker"in navigator&&(navigator.serviceWorker.controller&&navigator.serviceWorker.addEventListener("controllerchange",(function(){showNotification()})),window.addEventListener("load",(function(){navigator.serviceWorker.register("/sw.js")})))</script><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><script defer src="/js/diytitle.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,POWERMODE.mobile=!0,document.body.addEventListener("input",POWERMODE)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.4/metingjs/dist/Meting.min.js"></script><script>btf.addGlobalFn("pjaxSend",(()=>{if(window.aplayers)for(let a=0;a<window.aplayers.length;a++)window.aplayers[a].options.fixed||window.aplayers[a].destroy()}),"destroyAplayer"),btf.addGlobalFn("pjaxComplete",loadMeting,"runMetingJS")</script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script>(()=>{window.pjax=new Pjax({elements:'a:not([target="_blank"]):not([href="/talking/"]):not([href="/artitalk/"])',selectors:['link[rel="canonical"]','meta[property="og:image"]','meta[property="og:title"]','meta[property="og:url"]',"head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"],cacheBust:!1,analytics:!0,scrollRestoration:!1});const e=e=>{e&&Object.values(e).forEach((e=>e()))};document.addEventListener("pjax:send",(()=>{btf.removeGlobalFnEvent("pjaxSendOnce"),btf.removeGlobalFnEvent("themeChange");const t=document.body.classList;t.contains("read-mode")&&t.remove("read-mode"),e(window.globalFn.pjaxSend)})),document.addEventListener("pjax:complete",(()=>{btf.removeGlobalFnEvent("pjaxCompleteOnce"),document.querySelectorAll("script[data-pjax]").forEach((e=>{const t=document.createElement("script"),a=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach((e=>t.setAttribute(e.name,e.value))),t.appendChild(document.createTextNode(a)),e.parentNode.replaceChild(t,e)})),e(window.globalFn.pjaxComplete)})),document.addEventListener("pjax:error",(e=>{404===e.request.status&&pjax.loadUrl("/404")}))})()</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="algolia-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="search-wrap"><div id="algolia-search-input"></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-info"><div class="algolia-stats"></div><div class="algolia-poweredBy"></div></div></div></div></div><div id="search-mask"></div><script src="https://cdn.jsdelivr.net/npm/algoliasearch@5.12.0/dist/lite/builds/browser.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@4.75.3/dist/instantsearch.production.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-butterfly@5.2.2/source/js/search/algolia.min.js"></script></div></div><script data-pjax>if(document.getElementById("recent-posts")&&"/"==location.pathname){var parent=document.getElementById("recent-posts"),child='<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src= "/img/loading.gif" data-lazy-src="/img/nba-logo25.jpg" alt="/img/nba-logo25.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2024-02-02</span><a class="blog-slider__title" href="posts/1bcb0ad4/">单环境,多分支并行开发方案(流量染色/istio)</a><div class="blog-slider__text">单环境,多分支并行开发方案(流量染色/istio)</div><a class="blog-slider__button" href="posts/1bcb0ad4/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src= "/img/loading.gif" data-lazy-src="/img/nba-logo15.jpg" alt="/img/nba-logo15.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2023-04-06</span><a class="blog-slider__title" href="posts/2a9d73ff/">教你如何零成本从0到1，开发上线一个对接了openAI的机器人</a><div class="blog-slider__text">教你如何零成本从0到1，开发上线一个对接了openAI的机器人</div><a class="blog-slider__button" href="posts/2a9d73ff/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src= "/img/loading.gif" data-lazy-src="/img/nba-logo3.jpg" alt="/img/nba-logo3.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2023-06-14</span><a class="blog-slider__title" href="posts/aa43cc95/">记录k8s环境下结合alinode的使用</a><div class="blog-slider__text">记录k8s环境下结合alinode的使用</div><a class="blog-slider__button" href="posts/aa43cc95/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src= "/img/loading.gif" data-lazy-src="/img/nba-logo29.jpg" alt="/img/nba-logo29.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2022-12-13</span><a class="blog-slider__title" href="posts/15957791/">k8s环境下,nginx做websocket负载的方案梳理</a><div class="blog-slider__text">k8s环境下,nginx做websocket负载的方案梳理</div><a class="blog-slider__button" href="posts/15957791/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src= "/img/loading.gif" data-lazy-src="/img/nba-logo12.jpg" alt="/img/nba-logo12.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2023-09-26</span><a class="blog-slider__title" href="posts/4ecfb081/">记录一次k8s网络DNS问题排查过程</a><div class="blog-slider__text">记录一次k8s网络DNS问题排查过程</div><a class="blog-slider__button" href="posts/4ecfb081/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src= "/img/loading.gif" data-lazy-src="/img/nba-logo12.jpg" alt="/img/nba-logo12.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2023-09-26</span><a class="blog-slider__title" href="posts/37032a87/">k3s高可用安装</a><div class="blog-slider__text">k3s高可用安装</div><a class="blog-slider__button" href="posts/37032a87/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src= "/img/loading.gif" data-lazy-src="/img/nba-logo17.jpg" alt="/img/nba-logo17.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2022-11-27</span><a class="blog-slider__title" href="posts/20b19ed4/">分词搜索需求整理</a><div class="blog-slider__text">分词搜索需求整理</div><a class="blog-slider__button" href="posts/20b19ed4/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src= "/img/loading.gif" data-lazy-src="/img/nba-logo9.jpg" alt="/img/nba-logo9.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2022-12-02</span><a class="blog-slider__title" href="posts/6de14387/">整理wsl2配合开发的一些配置</a><div class="blog-slider__text">整理wsl2配合开发的一些配置</div><a class="blog-slider__button" href="posts/6de14387/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src= "/img/loading.gif" data-lazy-src="/img/nba-logo30.jpg" alt="/img/nba-logo30.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2021-12-11</span><a class="blog-slider__title" href="posts/fb9adcb6/">记录windows11+wsl2环境搭配</a><div class="blog-slider__text">windows11+wsl2开发环境配置</div><a class="blog-slider__button" href="posts/fb9adcb6/">详情</a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';console.log("已挂载swiper"),parent.insertAdjacentHTML("afterbegin",child)}</script><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper/swiper/swiper.min.js"></script><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper@0.18/swiper/swiperindex.js"></script><style></style><script data-pjax>function history_calendar_injector_config(){var i=document.getElementsByClassName("sticky_layout")[0];console.log("已挂载history_calendar"),i.insertAdjacentHTML("afterbegin",'<div class="card-widget card-history"><div class="card-content"><div class="item-headline"><i class="fas fa-clock fa-spin"></i><span>那年今日</span></div><div id="history-baidu" style="height: 100px;overflow: hidden"><div class="history_swiper-container" id="history-container" style="width: 100%;height: 100%"><div class="swiper-wrapper" id="history_container_wrapper" style="height:20px"></div></div></div></div>')}document.getElementsByClassName("sticky_layout")[0]&&"/"===location.pathname&&history_calendar_injector_config()</script><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/js/main.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({tagMode:!1,debug:!1,model:{jsonPath:"/live2dw/assets/wanko.model.json"},display:{position:"right",width:150,height:300,hOffset:20,vOffset:-20},mobile:{show:!0},log:!1,pluginJsPath:"lib/",pluginModelPath:"assets/",pluginRootPath:"live2dw/"})</script></body></html>